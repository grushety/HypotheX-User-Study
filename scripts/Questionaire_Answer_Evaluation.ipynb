{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# IMPORTS\n",
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "# Load JSON file (adjust path as needed in Colab)\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "file_name = list(uploaded.keys())[0]\n",
        "\n",
        "with open(file_name) as f:\n",
        "    data = json.load(f)"
      ],
      "metadata": {
        "id": "DHck61x-RZjy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JOj9UkE1-Uqk"
      },
      "outputs": [],
      "source": [
        "# Ground Truth Definitions\n",
        "ground_truths = {\n",
        "    \"hypothex\": {\n",
        "        \"importance\": [\"f5\", \"f4\", \"f1\", \"f2\", \"f3\"],\n",
        "        \"value_ranges\": {\n",
        "            \"f5\": (8, 15),  # Tool Worship >7\n",
        "            \"f4\": (0, 4),   # Box Collecting <4\n",
        "            \"f1\": (0, 5),   # LPC <5\n",
        "            \"f2\": (0, 5),   # RSS <5\n",
        "            \"f3\": (5, 8)    # PTP 5â€“8\n",
        "        },\n",
        "        \"main_profile\": \"Tool-Worship Focused Zarnak\",\n",
        "        \"sub_profiles\": [\n",
        "            \"Low Activity Zarnak\",\n",
        "            \"Balanced Worship + PTP Zarnak\"\n",
        "        ]\n",
        "    },\n",
        "    \"whatif\": {\n",
        "        \"importance\": [\"f5\", \"f2\", \"f1\", \"f3\", \"f4\"],\n",
        "        \"value_ranges\": {\n",
        "            \"f5\": (0, 4),    # Gardening < 4\n",
        "            \"f2\": (5, 10),   # Swimming 5â€“10\n",
        "            \"f1\": (5, 9),    # Running 5â€“9\n",
        "            \"f3\": (6, 12),   # Art 6â€“12\n",
        "            \"f4\": (7, 9)     # Detective Stories 7â€“9\n",
        "        },\n",
        "        \"main_profile\": \"General Duck Logic\",\n",
        "        \"sub_profiles\": [\n",
        "            \"Aquatic Nature Duck\",\n",
        "            \"Creative Aquatic Duck\",\n",
        "            \"Logical Low-Activity Duck\"\n",
        "        ]\n",
        "    }\n",
        "}\n",
        "\n",
        "# ðŸ§® Label â†’ numeric range mapping\n",
        "label_ranges = {\n",
        "    \"low\": (0, 5),\n",
        "    \"medium-low\": (0, 10),\n",
        "    \"medium\": (5, 10),\n",
        "    \"medium-high\": (5, 15),\n",
        "    \"high\": (10, 15)\n",
        "}\n",
        "\n",
        "# âš™ï¸ Metric Functions\n",
        "def score_importance_match(user_list, truth_list):\n",
        "    if not user_list:\n",
        "        return 0.0\n",
        "    jaccard = len(set(user_list).intersection(truth_list)) / len(set(user_list).union(truth_list))\n",
        "    order_penalty = sum([abs(i - truth_list.index(f)) for i, f in enumerate(user_list) if f in truth_list])\n",
        "    max_penalty = len(truth_list) * len(truth_list)\n",
        "    order_score = 1 - (order_penalty / max_penalty if max_penalty > 0 else 0)\n",
        "    return round((0.5 * jaccard + 0.5 * order_score), 2)\n",
        "\n",
        "def range_overlap_score(user_label, gt_range):\n",
        "    user_range = label_ranges.get(user_label.lower())\n",
        "    if not user_range or not gt_range:\n",
        "        return 0.0\n",
        "    u_start, u_end = user_range\n",
        "    g_start, g_end = gt_range\n",
        "\n",
        "    overlap_start = max(u_start, g_start)\n",
        "    overlap_end = min(u_end, g_end)\n",
        "    overlap = max(0, overlap_end - overlap_start)\n",
        "\n",
        "    user_len = u_end - u_start\n",
        "    return round(overlap / user_len, 2) if user_len > 0 else 0.0\n",
        "\n",
        "def score_value_direction_match(user_values, task):\n",
        "    if not user_values:\n",
        "        return 0.0\n",
        "    gt_ranges = ground_truths[task][\"value_ranges\"]\n",
        "    scores = []\n",
        "    for f, label in user_values.items():\n",
        "        gt_range = gt_ranges.get(f)\n",
        "        score = range_overlap_score(label, gt_range)\n",
        "        scores.append(score)\n",
        "    return round(sum(scores) / len(gt_ranges), 2) if scores else 0.0\n",
        "\n",
        "def profile_match_score(profile, main, subs):\n",
        "    if profile == main:\n",
        "        return 1.0\n",
        "    elif profile in subs:\n",
        "        return 0.5\n",
        "    return 0.0\n",
        "\n",
        "def overall_correctness_score(importance, value, profile):\n",
        "    return round(0.4 * importance + 0.4 * value + 0.2 * profile, 2)\n",
        "\n",
        "# Evaluation Loop\n",
        "results = []\n",
        "for task in [\"hypothex\", \"whatif\"]:\n",
        "    for pid, response in data[\"task1\"][task].items():\n",
        "        imp_score = score_importance_match(response[\"importance_order\"], ground_truths[task][\"importance\"])\n",
        "        val_score = score_value_direction_match(response[\"value_ranges\"], task)\n",
        "        prof_key = \"matches_profile\" if task == \"hypothex\" else \"matching_profile\"\n",
        "        prof_score = profile_match_score(response.get(prof_key), ground_truths[task][\"main_profile\"], ground_truths[task][\"sub_profiles\"])\n",
        "        overall = overall_correctness_score(imp_score, val_score, prof_score)\n",
        "\n",
        "        results.append({\n",
        "            \"tool_name\": task,\n",
        "            \"participant_id\": pid,\n",
        "            \"importance_score\": imp_score,\n",
        "            \"value_score\": val_score,\n",
        "            \"profile_score\": prof_score,\n",
        "            \"overall_score\": overall\n",
        "        })\n",
        "\n",
        "# ðŸ“Š Show Results\n",
        "df_task1 = pd.DataFrame(results)\n",
        "df_task1.sort_values(by=[\"tool_name\", \"participant_id\"], inplace=True)\n",
        "df_task1.reset_index(drop=True, inplace=True)\n",
        "df_task1.to_csv(\"evaluation_results_task_1.csv\", index=False)\n",
        "df_task1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ðŸ“Š STATISTICS PER TOOL AND PER PARTICIPANT\n",
        "\n",
        "# 1. Summary statistics per tool (mean scores across all participants)\n",
        "tool_summary = df_task1.groupby(\"tool_name\").agg({\n",
        "    \"importance_score\": \"mean\",\n",
        "    \"value_score\": \"mean\",\n",
        "    \"profile_score\": \"mean\",\n",
        "    \"overall_score\": \"mean\"\n",
        "}).reset_index()\n",
        "\n",
        "print(\"ðŸ“ˆ Average Scores per Tool:\")\n",
        "print(tool_summary)\n",
        "\n",
        "# Group correctness scores per participant for ANOVA\n",
        "participant_scores = df_task1.groupby(\"participant_id\")[\"overall_score\"].apply(list).reset_index()\n",
        "print(participant_scores)\n",
        "\n",
        "# 2. Per-participant overall score for each tool\n",
        "participant_summary = df_task1.pivot(index=\"participant_id\", columns=\"tool_name\", values=\"overall_score\").reset_index()\n",
        "participant_summary.columns.name = None  # Clean display\n",
        "\n",
        "print(\"\\nðŸ“Š Per-Participant Overall Scores (per Tool):\")\n",
        "print(participant_summary)\n"
      ],
      "metadata": {
        "id": "n69hiiLPI_wN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Define scoring functions ===\n",
        "def compute_correctness_score(labels):\n",
        "    label_weights = {\n",
        "        \"refutes_hypothesis\": 1.0,\n",
        "        \"works_in_combination\": 1.0,\n",
        "        \"weak_influence\": 1.0,\n",
        "        \"no_effect\": 0.75,\n",
        "        \"ambiguous\": 0.5\n",
        "    }\n",
        "    scores = [label_weights[lbl] for lbl in labels if lbl in label_weights]\n",
        "    return round(sum(scores) / len(scores), 2) if scores else 0.0\n",
        "\n",
        "\n",
        "def compute_analysis_score(evidence):\n",
        "    score = 0\n",
        "    if evidence.get(\"tested_multiple_classes\"):\n",
        "        score += 0.3\n",
        "    if evidence.get(\"tried_disconfirming_cases\"):\n",
        "        score += 0.2\n",
        "    if evidence.get(\"working_combination_found\"):\n",
        "        score += 0.3\n",
        "    if score > 0:\n",
        "        score += 0.2  # reasoning bonus\n",
        "    return min(score, 1.0)\n",
        "\n",
        "# === Build the result rows ===\n",
        "task2_results = []\n",
        "for tool in data[\"task2\"]:\n",
        "    for pid, entry in data[\"task2\"][tool].items():\n",
        "        labels = entry.get(\"hypothesis_validation_labels\", [])\n",
        "        evidence = entry.get(\"evidence_base\", {})\n",
        "        correctness = compute_correctness_score(labels)\n",
        "        analysis = compute_analysis_score(evidence)\n",
        "        final_score = round(0.4 * correctness + 0.6 * analysis, 2)\n",
        "\n",
        "        task2_results.append({\n",
        "            \"tool\": tool,\n",
        "            \"participant_id\": pid,\n",
        "            \"correctness_score\": correctness,\n",
        "            \"analysis_score\": analysis,\n",
        "            \"overall_score\": final_score\n",
        "        })\n",
        "\n",
        "# === Create output DataFrame ===\n",
        "df_task2 = pd.DataFrame(task2_results)\n",
        "df_task2.sort_values(by=[\"tool\", \"participant_id\"], inplace=True)\n",
        "df_task2.reset_index(drop=True, inplace=True)\n",
        "\n",
        "# === Save & show ===\n",
        "df_task2.to_csv(\"task2_scores_output.csv\", index=False)\n",
        "df_task2\n"
      ],
      "metadata": {
        "id": "bI46g-hbBlGg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ðŸ“Š 1. Average scores per tool\n",
        "tool_summary = df_task2.groupby(\"tool\").agg({\n",
        "    \"correctness_score\": \"mean\",\n",
        "    \"analysis_score\": \"mean\",\n",
        "    \"overall_score\": \"mean\"\n",
        "}).reset_index()\n",
        "\n",
        "print(\"ðŸ“ˆ Average Scores per Tool (Task 2):\")\n",
        "print(tool_summary)\n",
        "\n",
        "# ðŸ“Š 2. Per-participant final score for each tool\n",
        "participant_summary = df_task2.pivot(index=\"participant_id\", columns=\"tool\", values=\"overall_score\").reset_index()\n",
        "participant_summary.columns.name = None  # clean display\n",
        "\n",
        "print(\"\\nðŸ“Š Per-Participant Final Scores (Task 2):\")\n",
        "print(participant_summary)\n"
      ],
      "metadata": {
        "id": "YjBWzbWAGMik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# âš™ï¸ Metric Functions for Correctness and Complexity\n",
        "\n",
        "# Correctness Scoring based on the discrepancy of labels between truth and participant\n",
        "def calculate_correctness_score(ground_truth, participant_evaluation):\n",
        "    score_map = {\n",
        "        \"confirmed\": 0,\n",
        "        \"weakly_confirmed\": 0.25,\n",
        "        \"partially_disproved\": 0.5,\n",
        "        \"weakly_disproved\": 0.75,\n",
        "        \"disproved\": 1\n",
        "    }\n",
        "\n",
        "    # Get the numerical values for ground truth and participant evaluation\n",
        "    ground_value = score_map.get(ground_truth, 0)\n",
        "    participant_value = score_map.get(participant_evaluation, 0)\n",
        "\n",
        "    # Calculate the final score\n",
        "    return 1 - abs(ground_value - participant_value)\n",
        "\n",
        "\n",
        "# Complexity Evaluation based on weighted sum\n",
        "def calculate_complexity_score(hypothesis_property):\n",
        "    w_boundary = 0.4\n",
        "    w_range = 0.3\n",
        "    w_feature = 0.2\n",
        "    w_classes = 0.1\n",
        "\n",
        "    # Get the values from the hypothesis_property\n",
        "    boundary_score = hypothesis_property[\"number_of_boundary_values\"]\n",
        "    range_score = hypothesis_property[\"number_of_ranges_involved\"]\n",
        "    feature_score = hypothesis_property[\"number_of_features\"]\n",
        "    classes_score = hypothesis_property[\"number_of_classes\"]\n",
        "\n",
        "    # Calculate the weighted complexity score\n",
        "    complexity_score = (w_boundary * boundary_score +\n",
        "                        w_range * range_score +\n",
        "                        w_feature * feature_score +\n",
        "                        w_classes * classes_score)\n",
        "\n",
        "    # Define the maximum possible values for each component\n",
        "    max_boundary_value = 4  # max number of boundary values (example: >5, <4, etc.)\n",
        "    max_range_value = 3     # max number of ranges (example: low, medium, high)\n",
        "    max_feature_value = 5   # max number of features considered\n",
        "    max_class_value = 3     # max number of classes (example: Dog, Duck, Quorvian)\n",
        "\n",
        "    # Calculate the max possible complexity score (when all components are at their maximum)\n",
        "    max_possible_complexity = (w_boundary * max_boundary_value +\n",
        "                               w_range * max_range_value +\n",
        "                               w_feature * max_feature_value +\n",
        "                               w_classes * max_class_value) * 0.6\n",
        "\n",
        "    # Normalize the complexity score to a 0 - 1 range\n",
        "    normalized_complexity_score = complexity_score / max_possible_complexity\n",
        "\n",
        "    # Return the complexity level based on the normalized score\n",
        "    if normalized_complexity_score <= 0.33:\n",
        "        return \"Low Complexity\", normalized_complexity_score\n",
        "    elif normalized_complexity_score <= 0.66:\n",
        "        return \"Moderate Complexity\", normalized_complexity_score\n",
        "    else:\n",
        "        return \"High Complexity\", normalized_complexity_score\n",
        "\n",
        "\n",
        "# Evaluation Loop for Complexity and Correctness\n",
        "# Evaluation Loop for Complexity and Correctness\n",
        "results = []\n",
        "for tool in [\"hypothex\", \"whatif\"]:\n",
        "    for pid, response in data[\"task3\"][tool].items():\n",
        "        # Ensure the tool is explicitly added to the result\n",
        "        complexity_level, complexity_score = calculate_complexity_score(response[\"hypothesis_property\"])\n",
        "\n",
        "        # Calculate Correctness Score\n",
        "        ground_truth = response[\"hypothesis_validation_based_on_ground_true\"][\"value\"]\n",
        "        participant_evaluation = response[\"hypothesis_evaluation_by_participant\"][\"value\"]\n",
        "        correctness_score = calculate_correctness_score(ground_truth, participant_evaluation)\n",
        "\n",
        "        # Calculate Final Combined Score\n",
        "        final_score = (0.4 * complexity_score) + (0.6 * correctness_score)\n",
        "\n",
        "        # Add evaluation results to the list with the tool field\n",
        "        results.append({\n",
        "            \"tool\": tool,  # Explicitly add tool information\n",
        "            \"participant_id\": pid,\n",
        "            \"complexity_score\": complexity_score,\n",
        "            \"correctness_score\": correctness_score,\n",
        "            \"overall_score\": final_score,\n",
        "        })\n",
        "\n",
        "# ðŸ“Š Show Results\n",
        "df_task3 = pd.DataFrame(results)\n",
        "df_task3.sort_values(by=[\"tool\", \"participant_id\"], inplace=True)\n",
        "df_task3.reset_index(drop=True, inplace=True)\n",
        "df_task3.to_csv(\"evaluation_results_task_3.csv\", index=False)\n",
        "\n",
        "df_task3\n",
        "\n"
      ],
      "metadata": {
        "id": "jHE2RLsr2r1z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure that the columns have numeric values (not strings)\n",
        "df_task3[\"correctness_score\"] = pd.to_numeric(df_task3[\"correctness_score\"], errors=\"coerce\")\n",
        "df_task3[\"complexity_score\"] = pd.to_numeric(df_task3[\"complexity_score\"], errors=\"coerce\")\n",
        "df_task3[\"overall_score\"] = pd.to_numeric(df_task3[\"overall_score\"], errors=\"coerce\")\n",
        "\n",
        "# Now perform the groupby operation for average scores per tool\n",
        "tool_summary = df_task3.groupby(\"tool\").agg({\n",
        "    \"correctness_score\": \"mean\",\n",
        "    \"complexity_score\": \"mean\",\n",
        "    \"overall_score\": \"mean\"\n",
        "}).reset_index()\n",
        "\n",
        "print(\"ðŸ“ˆ Average Scores per Tool (Task 3):\")\n",
        "print(tool_summary)\n"
      ],
      "metadata": {
        "id": "O1cTLoNM-rnH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation Loop for Complexity and Correctness\n",
        "results = []\n",
        "for tool in [\"hypothex\", \"whatif\"]:\n",
        "    for pid, response in data[\"task4\"][tool].items():\n",
        "        # Ensure the tool is explicitly added to the result\n",
        "        complexity_level, complexity_score = calculate_complexity_score(response[\"hypothesis_property\"])\n",
        "\n",
        "        # Calculate Correctness Score\n",
        "        ground_truth = response[\"hypothesis_validation_based_on_ground_true\"][\"value\"]\n",
        "        participant_evaluation = response[\"hypothesis_evaluation_by_participant\"][\"value\"]\n",
        "        correctness_score = calculate_correctness_score(ground_truth, participant_evaluation)\n",
        "\n",
        "        # Calculate Final Combined Score\n",
        "        final_score = (0.4 * complexity_score) + (0.6 * correctness_score)\n",
        "\n",
        "        # Add evaluation results to the list with the tool field\n",
        "        results.append({\n",
        "            \"tool\": tool,  # Explicitly add tool information\n",
        "            \"participant_id\": pid,\n",
        "            \"complexity_score\": complexity_score,\n",
        "            \"correctness_score\": correctness_score,\n",
        "            \"overall_score\": final_score,\n",
        "        })\n",
        "\n",
        "# ðŸ“Š Show Results\n",
        "df_task4 = pd.DataFrame(results)\n",
        "df_task4.sort_values(by=[\"tool\", \"participant_id\"], inplace=True)\n",
        "df_task4.reset_index(drop=True, inplace=True)\n",
        "df_task4.to_csv(\"evaluation_results_task_4.csv\", index=False)\n",
        "\n",
        "df_task4\n",
        "\n"
      ],
      "metadata": {
        "id": "EWMx0sROWrkx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure that the columns have numeric values (not strings)\n",
        "df_task4[\"correctness_score\"] = pd.to_numeric(df_task4[\"correctness_score\"], errors=\"coerce\")\n",
        "df_task4[\"complexity_score\"] = pd.to_numeric(df_task4[\"complexity_score\"], errors=\"coerce\")\n",
        "df_task4[\"overall_score\"] = pd.to_numeric(df_task4[\"overall_score\"], errors=\"coerce\")\n",
        "\n",
        "# Now perform the groupby operation for average scores per tool\n",
        "tool_summary = df_task4.groupby(\"tool\").agg({\n",
        "    \"correctness_score\": \"mean\",\n",
        "    \"complexity_score\": \"mean\",\n",
        "    \"overall_score\": \"mean\"\n",
        "}).reset_index()\n",
        "\n",
        "print(\"ðŸ“ˆ Average Scores per Tool (Task 3):\")\n",
        "print(tool_summary)\n"
      ],
      "metadata": {
        "id": "slfI0cz2XJbZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example DataFrames with different columns\n",
        "# Assuming df_task1, df_task2, df_task3, df_task4 are already loaded and have different columns\n",
        "\n",
        "# Step 1: Get the union of all columns across all DataFrames\n",
        "all_columns = set(df_task1.columns).union(df_task2.columns).union(df_task3.columns).union(df_task4.columns)\n",
        "\n",
        "# Step 2: Ensure all DataFrames have the same columns by adding missing ones\n",
        "for df in [df_task1, df_task2, df_task3, df_task4]:\n",
        "    # Add missing columns with NaN values\n",
        "    for col in all_columns:\n",
        "        if col not in df.columns:\n",
        "            df[col] = pd.NA\n",
        "\n",
        "# Step 3: Concatenate all DataFrames into one, now they have the same columns\n",
        "df_all_tasks = pd.concat([df_task2, df_task3, df_task4], ignore_index=True)\n",
        "\n",
        "# Step 4: Optionally, ensure all numeric columns are converted to numeric if needed\n",
        "numeric_columns = ['correctness_score', 'complexity_score', 'overall_score']  # Adjust this list based on your columns\n",
        "for col in numeric_columns:\n",
        "    if col in df_all_tasks.columns:\n",
        "        df_all_tasks[col] = pd.to_numeric(df_all_tasks[col], errors=\"coerce\")\n",
        "\n",
        "# Step 5: Save the final DataFrame to a CSV file\n",
        "df_all_tasks.to_csv(\"2-4_combined_task_results.csv\", index=False)\n",
        "\n",
        "# ðŸ“Š Show the final DataFrame\n",
        "print(df_all_tasks)\n"
      ],
      "metadata": {
        "id": "jt4DoRUiPlgs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.formula.api import ols\n",
        "\n",
        "# Load combined results from tasks 2-4\n",
        "df = pd.read_csv(\"2-4_combined_task_results.csv\")\n",
        "\n",
        "# Filter necessary columns and drop missing values\n",
        "df = df[['participant_id', 'tool', 'correctness_score']].dropna()\n",
        "\n",
        "# Ensure correct data types\n",
        "df['participant_id'] = df['participant_id'].astype(str)\n",
        "df['tool'] = df['tool'].astype(str)\n",
        "df['correctness_score'] = pd.to_numeric(df['correctness_score'], errors='coerce')\n",
        "\n",
        "# Fit ANOVA model\n",
        "model = ols('correctness_score ~ C(tool) + C(participant_id)', data=df).fit()\n",
        "anova_table = sm.stats.anova_lm(model, typ=2)\n",
        "\n",
        "print(anova_table)\n"
      ],
      "metadata": {
        "id": "1vwS1BaQU9wa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}