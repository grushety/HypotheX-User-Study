{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Test preparation:  handle import and load files"
      ],
      "metadata": {
        "id": "b8HUgYnAZLeM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Install basic libraries\n",
        "!pip install pandas\n",
        "\n",
        "# --- Import libraries\n",
        "import math\n",
        "import os\n",
        "import io\n",
        "import re\n",
        "import json\n",
        "from google.colab import files\n",
        "import joblib\n",
        "\n",
        "from scipy import stats\n",
        "import pandas as pd\n",
        "from pandas.api.types import is_float_dtype\n",
        "import numpy as np\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.formula.api import ols\n",
        "from statsmodels.stats.anova import anova_lm\n",
        "\n",
        "from collections import defaultdict\n",
        "from collections import Counter\n",
        "from scipy.stats import entropy\n",
        "from IPython.display import display\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder"
      ],
      "metadata": {
        "id": "vc48Lcv1Q3kQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload multiple JSON files\n",
        "uploaded = files.upload()  # Opens file picker in Colab\n",
        "\n",
        "# Combine all uploaded JSONs into a single list of tasks\n",
        "all_tasks = []\n",
        "\n",
        "for filename in uploaded.keys():\n",
        "    try:\n",
        "        with open(filename, 'r', encoding='utf-8') as f:\n",
        "            data = json.load(f)\n",
        "            if isinstance(data, list):\n",
        "                all_tasks.extend(data)\n",
        "            else:\n",
        "                print(f\"Skipped {filename} (not a list at top level)\")\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to load {filename}: {e}\")\n",
        "\n",
        "# Keep only tasks that have at least one event\n",
        "all_tasks = [task for task in all_tasks if len(task.get(\"events\", [])) > 0]\n",
        "\n",
        "print(f\"Loaded {len(all_tasks)} total tasks from {len(uploaded)} files.\")"
      ],
      "metadata": {
        "id": "XOg4WPlRTcPh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Help function for plotting metrics   + function reused in other sections"
      ],
      "metadata": {
        "id": "BVU4syLkZhXT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sort_columns(df, tools_order=[\"WhatIf\", \"HypotheX\"]):\n",
        "    \"\"\"\n",
        "    Reorders metric columns so that for each metric, tools appear in tools_order.\n",
        "    Automatically ensures 'participant_id' and 'task_number' are retained and placed first if present.\n",
        "    \"\"\"\n",
        "    df = df.copy()\n",
        "    all_cols = df.columns.tolist()\n",
        "    metric_cols = [col for col in all_cols if \"_\" in col]\n",
        "    fixed_cols = [col for col in all_cols if \"_\" not in col and col not in [\"participant_id\", \"task_number\"]]\n",
        "\n",
        "    grouped = defaultdict(dict)\n",
        "    for col in metric_cols:\n",
        "        match = re.match(r\"(.+)_([A-Za-z0-9]+)$\", col)\n",
        "        if match:\n",
        "            metric, tool = match.groups()\n",
        "            grouped[metric][tool] = col\n",
        "\n",
        "    ordered_cols = []\n",
        "\n",
        "    # Insert with check to avoid duplicates\n",
        "    if \"participant_id\" in df.columns and \"participant_id\" not in ordered_cols:\n",
        "        ordered_cols.insert(0, \"participant_id\")\n",
        "    if \"task_number\" in df.columns and \"task_number\" not in ordered_cols:\n",
        "        ordered_cols.insert(1 if \"participant_id\" in ordered_cols else 0, \"task_number\")\n",
        "\n",
        "    ordered_cols.extend(fixed_cols)\n",
        "\n",
        "    for metric in sorted(grouped.keys()):\n",
        "        for tool in tools_order:\n",
        "            col = grouped[metric].get(tool)\n",
        "            if col:\n",
        "                ordered_cols.append(col)\n",
        "\n",
        "    return df[[col for col in ordered_cols if col in df.columns]]\n",
        "\n",
        "\n",
        "def get_safe_aggfuncs(df, default_func=\"mean\"):\n",
        "    \"\"\"\n",
        "    Returns a dictionary of column-specific aggregation functions,\n",
        "    but only for numeric columns. Object or unsupported types are skipped.\n",
        "    \"\"\"\n",
        "    return {\n",
        "        col: default_func\n",
        "        for col in df.columns\n",
        "        if pd.api.types.is_numeric_dtype(df[col])\n",
        "    }\n",
        "\n",
        "def get_metric_aggfunc_map(columns, default_func=\"mean\"):\n",
        "    \"\"\"\n",
        "    Assigns 'sum' to some metric names, 'mean' to the rest.\n",
        "    \"\"\"\n",
        "    always_sum_metrics = {\"hypothesis_count\", \"task_count\"}\n",
        "    return {\n",
        "        col: \"sum\" if col in always_sum_metrics else default_func\n",
        "        for col in columns\n",
        "    }\n",
        "\n",
        "def aggregate_precomputed_task_metrics(df, aggfuncs=None, tools_order=[\"WhatIf\", \"HypotheX\"]):\n",
        "    \"\"\"\n",
        "    Aggregates a precomputed task-level metrics DataFrame.\n",
        "    Assumes input columns include: participant_id, task_number, tool_name, and metrics.\n",
        "    Returns grouped summaries in original format: wide columns per tool per metric.\n",
        "    \"\"\"\n",
        "    if aggfuncs is None:\n",
        "        aggfuncs = {\n",
        "            \"tool_task\": \"sum\",\n",
        "            \"per_task\": \"mean\",\n",
        "            \"per_participant\": \"mean\",\n",
        "            \"per_tool\": \"mean\"\n",
        "        }\n",
        "\n",
        "    id_cols = {\"participant_id\", \"task_number\", \"tool_name\"}\n",
        "    metric_cols = [col for col in df.columns if col not in id_cols and pd.api.types.is_numeric_dtype(df[col])]\n",
        "\n",
        "    if not metric_cols:\n",
        "        raise ValueError(\"No numeric metric columns found for aggregation.\")\n",
        "\n",
        "    # Impute missing values only for numeric columns\n",
        "    df[metric_cols] = df[metric_cols].fillna(df[metric_cols].mean())  # Replace NaN values in numeric columns\n",
        "\n",
        "    # --- Raw participant-task view (not pivoted) ---\n",
        "    participant_task_df = df.sort_values(by=[\"participant_id\", \"task_number\", \"tool_name\"])\n",
        "\n",
        "    metric_aggfuncs = get_metric_aggfunc_map(metric_cols, default_func=\"mean\")\n",
        "\n",
        "    # --- Tool-task view: pivoted wide format ---\n",
        "    grouped_tool = (\n",
        "        df.groupby([\"task_number\", \"tool_name\"])[metric_cols]\n",
        "        .agg(metric_aggfuncs)\n",
        "        .reset_index()\n",
        "    )\n",
        "\n",
        "    tool_task_df = grouped_tool.pivot(index=\"task_number\", columns=\"tool_name\", values=metric_cols)\n",
        "    tool_task_df.columns = [f\"{metric}_{tool}\" for metric, tool in tool_task_df.columns]\n",
        "    tool_task_df = tool_task_df.reset_index()\n",
        "    tool_task_df = sort_columns(tool_task_df, tools_order)\n",
        "\n",
        "    # --- Per-task view (mean over all tools and participants) ---\n",
        "    per_task_df = (\n",
        "        df.groupby(\"task_number\")[metric_cols]\n",
        "        .agg(metric_aggfuncs)\n",
        "        .reset_index()\n",
        "        .sort_values(by=\"task_number\")\n",
        "    )\n",
        "\n",
        "    # --- Per-participant view: pivoted wide format ---\n",
        "    grouped_participant = (\n",
        "        df.groupby([\"participant_id\", \"tool_name\"])[metric_cols]\n",
        "        .agg(metric_aggfuncs)\n",
        "        .reset_index()\n",
        "    )\n",
        "\n",
        "    per_participant_df = grouped_participant.pivot(index=\"participant_id\", columns=\"tool_name\", values=metric_cols)\n",
        "    per_participant_df.columns = [f\"{metric}_{tool}\" for metric, tool in per_participant_df.columns]\n",
        "    per_participant_df = per_participant_df.reset_index()\n",
        "    per_participant_df = sort_columns(per_participant_df, tools_order)\n",
        "\n",
        "    # --- Per-tool view (mean over all tasks and participants) ---\n",
        "    per_tool_df = (\n",
        "        df.groupby(\"tool_name\")[metric_cols]\n",
        "        .agg(metric_aggfuncs)\n",
        "        .reset_index()\n",
        "        .sort_values(by=\"tool_name\")\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        \"participant_task\": participant_task_df,\n",
        "        \"tool_task\": tool_task_df,\n",
        "        \"per_task\": per_task_df,\n",
        "        \"per_participant\": per_participant_df,\n",
        "        \"per_tool\": per_tool_df\n",
        "    }\n",
        "\n",
        "def format_integer_columns(df):\n",
        "    \"\"\"\n",
        "    Converts float columns with all integer values to int dtype for cleaner display.\n",
        "    \"\"\"\n",
        "    for col in df.select_dtypes(include=\"float\"):\n",
        "        if df[col].dropna().apply(float.is_integer).all():\n",
        "            df[col] = df[col].astype(\"Int64\")  # nullable integer type\n",
        "    return df\n",
        "\n",
        "\n",
        "def format_summary_columns(summary_dict):\n",
        "    \"\"\"\n",
        "    Format all summary tables:\n",
        "    - Abbreviate metric/tool names (e.g. confirmation_ratio_WhatIf â†’ CAR_wit)\n",
        "    - Convert floats: proportions to %, ints without .0\n",
        "    - Replace NaNs with \"-\"\n",
        "    \"\"\"\n",
        "    from pandas.api.types import is_float_dtype\n",
        "\n",
        "    METRIC_ABBREVIATIONS = {\n",
        "        \"contradiction_ignoring_index\": \"CII\",\n",
        "        \"contradictory_evidence_ratio\": \"CER\",\n",
        "        \"confirmation_strategy_entropy\": \"CSE\",\n",
        "        \"confirmed_hypothesis_action_agreement_ratio\": \"CHAAR\",\n",
        "        \"disproved_hypothesis_action_agreement_ratio\": \"DHAAR\",\n",
        "        \"main_profile_reached\": \"MPR\",\n",
        "        \"subgroup_coverage\": \"SubC\",\n",
        "        \"multiclass_selection_entropy\": \"MSelE\",\n",
        "        \"class_strategy_entropy\": \"CSE\",\n",
        "        \"feature_coverage\": \"FC\",\n",
        "        \"feature_test_balance\": \"FTB\",\n",
        "        \"goal_class_feature_range_coverage\": \"FC\",\n",
        "        \"early_evaluation_consistency\": \"EEC\",\n",
        "        \"late_evaluation_influence\": \"LEC\",\n",
        "        \"average_hypothesis_complexity\": \"HC\",\n",
        "        \"hypothesis_success_rate\": \"HSR\",\n",
        "        \"task_duration_minutes\": \"AvTaskTime\",\n",
        "        \"inspect_before_modify_ratio\": \"IBM_R\",\n",
        "    }\n",
        "\n",
        "    TOOL_ABBREVIATIONS = {\n",
        "        \"WhatIf\": \"wit\",\n",
        "        \"HypotheX\": \"HX\"\n",
        "    }\n",
        "\n",
        "    METADATA_COLUMNS = {\"participant_id\", \"task_number\", \"tool_name\"}\n",
        "\n",
        "    formatted = {}\n",
        "\n",
        "    for name, df in summary_dict.items():\n",
        "        df = df.copy()\n",
        "\n",
        "        # Flatten MultiIndex if necessary\n",
        "        if isinstance(df.columns, pd.MultiIndex):\n",
        "            df.columns = ['_'.join([str(level) for level in col if level]) for col in df.columns]\n",
        "\n",
        "        # Format numerical columns\n",
        "        for col in df.columns:\n",
        "            if is_float_dtype(df[col]):\n",
        "                col_data = df[col].dropna()\n",
        "\n",
        "                if col_data.apply(float.is_integer).all():\n",
        "                    df[col] = df[col].astype(\"Int64\").astype(str)\n",
        "                else:\n",
        "                    df[col] = df[col].round(2).astype(str)\n",
        "\n",
        "            df[col] = df[col].replace([\"nan\", \"NaN\", \"None\", \"nan%\"], \"-\").fillna(\"-\")\n",
        "\n",
        "         # --- Build rename map ---\n",
        "        rename_map = {}\n",
        "        for col in df.columns:\n",
        "            if col in METRIC_ABBREVIATIONS:\n",
        "                rename_map[col] = METRIC_ABBREVIATIONS[col]\n",
        "            elif \"_\" in col:\n",
        "                metric, tool = col.rsplit(\"_\", 1)\n",
        "                if metric in METRIC_ABBREVIATIONS:\n",
        "                    abbrev_metric = METRIC_ABBREVIATIONS[metric]\n",
        "                    abbrev_tool = TOOL_ABBREVIATIONS.get(tool, tool)\n",
        "                    rename_map[col] = f\"{abbrev_metric}_{abbrev_tool}\"\n",
        "\n",
        "\n",
        "        df = df.rename(columns=rename_map)\n",
        "        formatted[name] = df\n",
        "\n",
        "    return formatted\n",
        "\n",
        "\n",
        "\n",
        "def compute_entropy(values):\n",
        "    \"\"\"\n",
        "    Computes entropy of a list of discrete values.\n",
        "    \"\"\"\n",
        "    if not values:\n",
        "        return None\n",
        "    counts = Counter(values)\n",
        "    total = sum(counts.values())\n",
        "    probs = [count / total for count in counts.values()]\n",
        "    return round(-sum(p * math.log2(p) for p in probs), 6)\n",
        "\n",
        "def extract_declared_features(text):\n",
        "    \"\"\"\n",
        "    Extracts declared feature IDs (e.g., f1, f2, etc.) from hypothesis description.\n",
        "    \"\"\"\n",
        "    if not text:\n",
        "        return []\n",
        "    return re.findall(r\"\\bf\\d+\\b\", text.lower())\n",
        "\n",
        "def perform_one_way_anova(df, metrics=[\"average_hypothesis_complexity\"], factor=\"tool_name\"):\n",
        "    \"\"\"\n",
        "    Perform One-Way ANOVA for each metric between the tools (WhatIf and HypotheX) for each participant and task.\n",
        "\n",
        "    Parameters:\n",
        "    df (pd.DataFrame): DataFrame containing the participant data for each tool.\n",
        "    metrics (list): List of metrics to perform ANOVA on (e.g., ['TC', 'EC', 'average_hypothesis_complexity']).\n",
        "    factor (str): The factor for ANOVA (e.g., \"tool_name\" or \"participant_id\").\n",
        "\n",
        "    Returns:\n",
        "    pd.DataFrame: A DataFrame containing ANOVA results for each metric.\n",
        "    \"\"\"\n",
        "    # Store results for each metric in a dictionary\n",
        "    anova_results = []\n",
        "\n",
        "    # Strip whitespace from column names\n",
        "    df.columns = df.columns.str.strip()\n",
        "\n",
        "    # Loop through each metric in the metrics list\n",
        "    for metric in metrics:\n",
        "        metric = metric.strip()  # Ensure the metric name is clean of spaces\n",
        "\n",
        "        if metric not in df.columns:\n",
        "            print(f\"Warning: Metric '{metric}' not found in the DataFrame.\")\n",
        "            continue\n",
        "\n",
        "        # Check if there is variation in the metric (all values are not the same)\n",
        "        if df[metric].nunique() < 2:\n",
        "            print(f\"Warning: Not enough variation in the metric '{metric}' to perform ANOVA.\")\n",
        "            continue\n",
        "\n",
        "        # Filter data based on the factor (e.g., 'tool_name' or 'participant_id')\n",
        "        #whatif_data = df[df[factor] == 'WhatIf'][metric]\n",
        "        #hypothex_data = df[df[factor] == 'HypotheX'][metric]\n",
        "\n",
        "        whatif_data = df[df[factor].str.lower() == 'whatif'.lower()][metric]\n",
        "        hypothex_data = df[df[factor].str.lower() == 'hypothex'.lower()][metric]\n",
        "\n",
        "\n",
        "        # Ensure both groups have at least 2 samples for the ANOVA\n",
        "        if len(whatif_data) < 2 or len(hypothex_data) < 2:\n",
        "            print(f\"Warning: Not enough data for ANOVA for {metric} between WhatIf and HypotheX.\")\n",
        "            continue\n",
        "\n",
        "        # Perform One-Way ANOVA for each metric\n",
        "        formula = f\"{metric} ~ C({factor})\"\n",
        "        model = ols(formula, data=df).fit()\n",
        "        anova_table = anova_lm(model)\n",
        "\n",
        "        # Extract the F-statistic and P-value\n",
        "        f_statistic = anova_table[\"F\"][0] if \"F\" in anova_table else None\n",
        "        p_value = anova_table[\"PR(>F)\"][0] if \"PR(>F)\" in anova_table else None\n",
        "\n",
        "        # Handle case for missing values (e.g., NaN)\n",
        "        if pd.isna(f_statistic) or pd.isna(p_value):\n",
        "            f_statistic = \"NaN\"\n",
        "            p_value = \"NaN\"\n",
        "            print(f\"Warning: ANOVA returned NaN for {metric}.\")\n",
        "\n",
        "        # Determine the meaning of the P-value\n",
        "        if isinstance(p_value, (int, float)) and p_value < 0.05:\n",
        "            significance = \"yes\"\n",
        "        else:\n",
        "            significance = \"no\"\n",
        "\n",
        "        # Add the results to the list\n",
        "        anova_results.append({\n",
        "            \"Metric Name\": metric,\n",
        "            \"F-statistic\": f_statistic,\n",
        "            \"Statistical Significance\": significance,\n",
        "            \"P-value\": p_value,\n",
        "            \"P-value Meaning\": significance\n",
        "        })\n",
        "\n",
        "    # Convert the results into a DataFrame\n",
        "    anova_results_df = pd.DataFrame(anova_results)\n",
        "\n",
        "    return anova_results_df\n",
        "\n",
        "\n",
        "def perform_two_way_anova(df, metrics=[\"average_hypothesis_complexity\"], factors=[\"tool_name\", \"task_number\"]):\n",
        "    \"\"\"\n",
        "    Perform Two-Way ANOVA for multiple metrics with specified factors.\n",
        "    Supports combinations of 'tool_name' and 'task_number', or 'tool_name' and 'participant_id'.\n",
        "\n",
        "    Parameters:\n",
        "    df (pd.DataFrame): DataFrame containing the participant data.\n",
        "    metrics (list): List of metrics to perform ANOVA on (e.g., ['TC', 'EC', 'average_hypothesis_complexity']).\n",
        "    factors (list): List of two factors for ANOVA (e.g., ['tool_name', 'task_number'] or ['tool_name', 'participant_id']).\n",
        "\n",
        "    Returns:\n",
        "    pd.DataFrame: A DataFrame containing ANOVA results for each metric.\n",
        "    \"\"\"\n",
        "    # Validate the factors length (should be 2 for two-way ANOVA)\n",
        "    if len(factors) != 2:\n",
        "        raise ValueError(\"Two factors are required for Two-Way ANOVA. Use factors=['factor1', 'factor2'].\")\n",
        "\n",
        "    factor_1, factor_2 = factors\n",
        "\n",
        "    # Store results for all metrics in a list\n",
        "    anova_results = []\n",
        "\n",
        "    # Loop through each metric in the metrics list\n",
        "    for metric in metrics:\n",
        "        metric = metric.strip()  # Ensure the metric name is clean of spaces\n",
        "\n",
        "        if metric not in df.columns:\n",
        "            print(f\"Warning: Metric '{metric}' not found in the DataFrame.\")\n",
        "            continue\n",
        "\n",
        "        # Check if there is variation in the metric (all values are not the same)\n",
        "        if df[metric].nunique() < 2:\n",
        "            print(f\"Warning: Not enough variation in the metric '{metric}' to perform ANOVA.\")\n",
        "            continue\n",
        "\n",
        "        # Perform Two-Way ANOVA for each metric\n",
        "        formula = f\"{metric} ~ C({factor_1}) + C({factor_2}) + C({factor_1}):C({factor_2})\"\n",
        "        model = ols(formula, data=df).fit()\n",
        "        anova_table = anova_lm(model)\n",
        "\n",
        "        # Access the interaction term row explicitly using .iloc() or .loc()\n",
        "        interaction_row = anova_table.iloc[2]  # Interaction term\n",
        "\n",
        "        # Extract the F-statistic and P-value\n",
        "        f_statistic = interaction_row[\"F\"]\n",
        "        p_value = interaction_row[\"PR(>F)\"]\n",
        "\n",
        "        # Handle case for missing values (e.g., NaN)\n",
        "        if pd.isna(f_statistic) or pd.isna(p_value):\n",
        "            f_statistic = \"NaN\"\n",
        "            p_value = \"NaN\"\n",
        "            print(f\"Warning: ANOVA returned NaN for {metric}.\")\n",
        "\n",
        "        # Determine the meaning of the P-value\n",
        "        if isinstance(p_value, (int, float)) and p_value < 0.05:\n",
        "            significance = \"yes\"\n",
        "        else:\n",
        "            significance = \"no\"\n",
        "\n",
        "        # Add the results to the list\n",
        "        anova_results.append({\n",
        "            \"Metric Name\": metric,\n",
        "            \"F-statistic\": f_statistic,\n",
        "            \"Statistical Significance\": significance,\n",
        "            \"P-value\": p_value,\n",
        "            \"P-value Meaning\": significance\n",
        "        })\n",
        "\n",
        "    # Convert the results into a DataFrame\n",
        "    anova_results_df = pd.DataFrame(anova_results)\n",
        "\n",
        "    return anova_results_df\n",
        "\n",
        "\n",
        "def compute_all_task_metrics(all_tasks, all_metric_functions):\n",
        "    \"\"\"\n",
        "    Computes all specified metrics per task.\n",
        "    Returns a flat DataFrame: one row per task, one column per metric.\n",
        "    \"\"\"\n",
        "    import pandas as pd\n",
        "\n",
        "    rows = []\n",
        "\n",
        "    for task in all_tasks:\n",
        "        row = {\n",
        "            \"participant_id\": task.get(\"participant_id\"),\n",
        "            \"task_number\": task.get(\"task_number\"),\n",
        "            \"tool_name\": task.get(\"tool_name\")\n",
        "        }\n",
        "\n",
        "        for metric_name, metric_func in all_metric_functions.items():\n",
        "            try:\n",
        "                row[metric_name] = metric_func(task)\n",
        "            except Exception:\n",
        "                row[metric_name] = None  # fallback for errors\n",
        "\n",
        "        rows.append(row)\n",
        "\n",
        "    return pd.DataFrame(rows)"
      ],
      "metadata": {
        "id": "cneIxlLyM622"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preparing models and dataset information for each tool"
      ],
      "metadata": {
        "id": "oWOzLgIFefmV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_and_prepare_tool_data():\n",
        "    \"\"\"\n",
        "    Loads all necessary files, performs scaling and inference, and returns:\n",
        "    - df_raw: original dataset\n",
        "    - df_scaled: scaled features\n",
        "    - model: loaded model\n",
        "    - label_encoder: loaded encoder\n",
        "    - predictions: predicted class labels\n",
        "    - probabilities: predicted class probabilities\n",
        "    \"\"\"\n",
        "    label_column = \"Class\"\n",
        "    uploaded_files = files.upload()\n",
        "\n",
        "    dataset_filename = model_filename = scaler_filename = label_encoder_filename = None\n",
        "\n",
        "    for filename in uploaded_files:\n",
        "        if filename.endswith(\".csv\"):\n",
        "            dataset_filename = filename\n",
        "        elif \"model\" in filename and filename.endswith(\".pkl\"):\n",
        "            model_filename = filename\n",
        "        elif \"scaler\" in filename and filename.endswith(\".pkl\"):\n",
        "            scaler_filename = filename\n",
        "        elif \"label_encoder\" in filename and filename.endswith(\".pkl\"):\n",
        "            label_encoder_filename = filename\n",
        "\n",
        "    # Load files\n",
        "    df = pd.read_csv(dataset_filename)\n",
        "    print(f\"âœ… Loaded dataset: {dataset_filename}\")\n",
        "\n",
        "    model = joblib.load(model_filename)\n",
        "    print(f\"âœ… Loaded model: {model_filename}\")\n",
        "\n",
        "    scaler = joblib.load(scaler_filename)\n",
        "    print(f\"âœ… Loaded scaler: {scaler_filename}\")\n",
        "\n",
        "    label_encoder = joblib.load(label_encoder_filename)\n",
        "    print(f\"âœ… Loaded label encoder: {label_encoder_filename}\")\n",
        "\n",
        "    # Apply scaler\n",
        "    feature_columns = [col for col in df.columns if col != label_column]\n",
        "    X_scaled = scaler.transform(df[feature_columns])\n",
        "    df_scaled = pd.DataFrame(X_scaled, columns=feature_columns, index=df.index)\n",
        "\n",
        "    # Inference\n",
        "    predictions = model.predict(X_scaled)\n",
        "    probabilities = model.predict_proba(X_scaled)\n",
        "\n",
        "    print(\"âœ… Scaling, encoding, and inference complete.\")\n",
        "\n",
        "    return df, df_scaled, predictions, probabilities, label_encoder\n",
        "\n",
        "\n",
        "def setup_tool_model_structure(df_raw, df_scaled, pred_classes, probs, label_encoder, tool_name, max_k=5, label_column = \"Class\"):\n",
        "    \"\"\"\n",
        "    For a given tool:\n",
        "    1. Maps point_id to prediction and probability\n",
        "    2. Runs k-means clustering per predicted class\n",
        "    3. Stores cluster profiles, main and high probability clusters\n",
        "    \"\"\"\n",
        "\n",
        "    # Prepare augmented DataFrame\n",
        "    df_raw = df_raw.copy()\n",
        "    df_raw[\"predicted_class\"] = pred_classes\n",
        "    df_raw[\"point_id\"] = df_raw.index\n",
        "    df_raw[\"predicted_prob\"] = probs.max(axis=1)\n",
        "    prob_df = pd.DataFrame(probs, columns=[f\"prob_class_{i}\" for i in range(probs.shape[1])])\n",
        "    df_full = pd.concat([df_raw.reset_index(drop=True), prob_df], axis=1)\n",
        "\n",
        "    # Encode original class labels (optional, for mapping)\n",
        "    true_classes = label_encoder.fit_transform(df_raw[label_column])\n",
        "    class_mapping = {i: label for i, label in enumerate(label_encoder.classes_)}\n",
        "    feature_columns = [col for col in df_raw.columns if col not in [\"Class\", \"predicted_class\", \"predicted_prob\", \"point_id\"] and not col.startswith(\"prob_class_\")]\n",
        "\n",
        "    # Prepare return structure\n",
        "    profiles = {\n",
        "        \"point_id_to_class\": df_full.set_index(\"point_id\")[\"predicted_class\"].to_dict(),\n",
        "        \"point_id_to_prob\": df_full.set_index(\"point_id\")[\"predicted_prob\"].to_dict(),\n",
        "        \"point_id_to_full_probs\": df_full[[\"point_id\"] + [f\"prob_class_{i}\" for i in range(probs.shape[1])]].set_index(\"point_id\").to_dict(\"index\"),\n",
        "        \"clusters\": {},         # cluster_id per class\n",
        "        \"main_profile\": {},    # largest cluster\n",
        "        \"high_profile\": {},     # cluster with highest avg probability\n",
        "        \"class_feature_spaces\": {},  # max feature ranges for one class\n",
        "        \"label_conversion_map\": {},\n",
        "        \"point_id_to_features\": {},\n",
        "    }\n",
        "\n",
        "    # Store per-class feature values\n",
        "    class_feature_spaces = {}\n",
        "    for class_idx in np.unique(pred_classes):\n",
        "        point_ids = [i for i, pred in enumerate(pred_classes) if pred == class_idx]\n",
        "        class_feature_spaces[class_idx] = [df_raw.iloc[i][feature_columns].to_dict() for i in point_ids]\n",
        "    profiles[\"class_feature_spaces\"] = class_feature_spaces\n",
        "\n",
        "    profiles[\"point_id_to_features\"] = df_raw[feature_columns].to_dict(\"index\")\n",
        "\n",
        "    # This is fixed knowledge about the semantic meaning of c1, c2, c3\n",
        "    tool_semantic_c_map = {\n",
        "        \"WhatIf\": {\n",
        "            \"duck\": \"c1\",\n",
        "            \"cat\": \"c2\",\n",
        "            \"dog\": \"c3\"\n",
        "        },\n",
        "        \"HypotheX\": {\n",
        "            \"zarnak\": \"c1\",\n",
        "            \"bliptor\": \"c2\",\n",
        "            \"quorvian\": \"c3\"\n",
        "        }\n",
        "    }\n",
        "\n",
        "    semantic_map = tool_semantic_c_map.get(tool_name, {})\n",
        "    label_conversion_map = {}\n",
        "\n",
        "    for label in label_encoder.classes_:\n",
        "        index = int(label_encoder.transform([label])[0])\n",
        "        label_low = label.lower()\n",
        "        # Assign also c1/c2/c3 mapping\n",
        "        if label_low in semantic_map:\n",
        "            cx = semantic_map[label_low]\n",
        "            label_conversion_map[cx] = index\n",
        "    profiles[\"label_conversion_map\"] = label_conversion_map\n",
        "\n",
        "    # Run KMeans per predicted class\n",
        "    for c in np.unique(pred_classes):\n",
        "        class_df = df_scaled[pred_classes == c]\n",
        "        if len(class_df) < 3:\n",
        "            continue\n",
        "\n",
        "        best_k = 2\n",
        "        max_diff = -np.inf\n",
        "        best_labels = None\n",
        "\n",
        "        # Try k=2 to max_k clusters to maximize probability separation\n",
        "        for k in range(2, min(max_k, len(class_df)) + 1):\n",
        "            kmeans = KMeans(n_clusters=k, random_state=42).fit(class_df)\n",
        "            labels = kmeans.labels_\n",
        "\n",
        "            temp_df = pd.DataFrame({\n",
        "                \"cluster\": labels,\n",
        "                \"predicted_prob\": probs[pred_classes == c, c]\n",
        "            })\n",
        "\n",
        "            mean_probs = temp_df.groupby(\"cluster\")[\"predicted_prob\"].mean()\n",
        "            diff = mean_probs.max() - mean_probs.min()\n",
        "\n",
        "            if diff > max_diff:\n",
        "                max_diff = diff\n",
        "                best_k = k\n",
        "                best_labels = labels\n",
        "\n",
        "        # Assign cluster labels to point_ids\n",
        "        class_point_ids = df_raw[pred_classes == c].index\n",
        "        cluster_map = dict(zip(class_point_ids, best_labels))\n",
        "        profiles[\"clusters\"][int(c)] = cluster_map\n",
        "\n",
        "        # Main profile: largest cluster\n",
        "        largest_cluster = pd.Series(best_labels).value_counts().idxmax()\n",
        "        profiles[\"main_profile\"][int(c)] = [pid for pid, cl in cluster_map.items() if cl == largest_cluster]\n",
        "\n",
        "        # High profile: cluster with highest avg predicted prob\n",
        "        temp_df = pd.DataFrame({\n",
        "            \"point_id\": list(class_point_ids),\n",
        "            \"cluster\": best_labels,\n",
        "            \"prob\": probs[pred_classes == c, c]\n",
        "        })\n",
        "        high_cluster = temp_df.groupby(\"cluster\")[\"prob\"].mean().idxmax()\n",
        "        profiles[\"high_profile\"][int(c)] = temp_df[temp_df[\"cluster\"] == high_cluster][\"point_id\"].tolist()\n",
        "    return profiles"
      ],
      "metadata": {
        "id": "MENHGdJVeetl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload all file from Animal Hobbies Dataset and RF model, also scaler and label_encode\n",
        "\n",
        "tool_profiles = {}\n",
        "wit_df, wit_scaled, wit_preds, wit_probs, wit_encoder = load_and_prepare_tool_data()"
      ],
      "metadata": {
        "id": "yVRxGGKlANfW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload all file from Animal Hobbies Dataset and RF model, also scaler and label_encode\n",
        "\n",
        "hyp_df, hyp_scaled, hyp_preds, hyp_probs, hyp_encoder = load_and_prepare_tool_data()"
      ],
      "metadata": {
        "id": "Tz_N4yOEFm2Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### add all file from Animal Hobbies Dataset and RF model, also scaler and label_encode\n",
        "# Process model structure\n",
        "tool_profiles[\"WhatIf\"] = setup_tool_model_structure(\n",
        "    df_raw=wit_df,\n",
        "    df_scaled=wit_scaled,\n",
        "    pred_classes=wit_preds,\n",
        "    probs=wit_probs,\n",
        "    label_encoder = wit_encoder,\n",
        "    tool_name = \"WhatIf\"\n",
        ")\n",
        "# Process model structure\n",
        "tool_profiles[\"HypotheX\"] = setup_tool_model_structure(\n",
        "    df_raw=hyp_df,\n",
        "    df_scaled=hyp_scaled,\n",
        "    pred_classes=hyp_preds,\n",
        "    probs=hyp_probs,\n",
        "    label_encoder = hyp_encoder,\n",
        "    tool_name = \"HypotheX\"\n",
        ")\n",
        "\n",
        "print(tool_profiles[\"WhatIf\"]['label_conversion_map'])"
      ],
      "metadata": {
        "id": "_t69wuXrFFjz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Basic metrics"
      ],
      "metadata": {
        "id": "VDW2blG5Zv0L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#################################\n",
        "#     BASIC STATS FUNCTIONS     #\n",
        "#################################\n",
        "def average_hypothesis_complexity(task):\n",
        "    \"\"\"\n",
        "    Returns average complexity score of all hypotheses in a task.\n",
        "    Complexity is based on number of features, range cues, boundary cues, and class mentions.\n",
        "    \"\"\"\n",
        "    def extract_declared_features(text):\n",
        "        if not text:\n",
        "            return []\n",
        "        return re.findall(r\"\\bf\\d+\\b\", text.lower())\n",
        "\n",
        "    def compute_complexity(desc):\n",
        "        desc = desc.lower()\n",
        "\n",
        "        # Features\n",
        "        features = set(extract_declared_features(desc))\n",
        "        num_features = len(features)\n",
        "\n",
        "        # Ranges\n",
        "        range_patterns = [\n",
        "            r\"between\\s+\\d+\\s+(and|to)\\s+\\d+\",\n",
        "            r\"from\\s+\\d+\\s+(to|-)\\s+\\d+\",\n",
        "            r\"\\b\\d+\\s*-\\s*\\d+\\b\"\n",
        "        ]\n",
        "        range_count = sum(len(re.findall(p, desc)) for p in range_patterns)\n",
        "\n",
        "        qual_range_words = [\n",
        "            \"high\", \"higher\", \"like\", \"increase\", \"increasing\",\n",
        "            \"low\", \"lower\", \"don't like\", \"decrease\", \"decreasing\", \"avoid\", \"dislike\",\n",
        "            \"medium\", \"middle\", \"moderate\", \"smaller\"\n",
        "        ]\n",
        "        range_count += sum(len(re.findall(rf\"\\b{word}\\b\", desc)) for word in qual_range_words)\n",
        "\n",
        "        # Boundaries\n",
        "        boundary_patterns = [\n",
        "            r\"f\\d+\\s*(>|<|>=|<=)\\s*\\d+\",\n",
        "            r\"more than\\s+\\d+\",\n",
        "            r\"less than\\s+\\d+\"\n",
        "        ]\n",
        "        boundary_count = sum(len(re.findall(p, desc)) for p in boundary_patterns)\n",
        "\n",
        "        # Class mentions\n",
        "        class_keywords = [\"c1\", \"c2\", \"c3\", \"duck\", \"dog\", \"cat\", \"bliptor\", \"zarnak\", \"quorvian\"]\n",
        "        class_count = sum(1 for c in class_keywords if re.search(rf\"\\b{c}\\b\", desc))\n",
        "\n",
        "        # Normalized components\n",
        "        feature_score = min(num_features / 5, 1.0)\n",
        "        range_score = min(range_count / 5, 1.0)\n",
        "        boundary_score = min(boundary_count / 5, 1.0)\n",
        "        class_score = min(class_count / 3, 1.0)\n",
        "\n",
        "        return round((feature_score + range_score + boundary_score + class_score) / 4, 3)\n",
        "\n",
        "    scores = [\n",
        "        compute_complexity(h.get(\"description\", \"\"))\n",
        "        for h in task.get(\"events\", [])\n",
        "        if h.get(\"event_class\") == \"Hypothesis\"\n",
        "\n",
        "\n",
        "    ]\n",
        "\n",
        "    return round(sum(scores) / len(scores), 3) if scores else None\n",
        "\n",
        "def second_order_event_count(task):\n",
        "    \"\"\"\n",
        "    Counts only second-order events nested under Hypothesis-type events.\n",
        "    \"\"\"\n",
        "    count = 0\n",
        "    for event in task.get(\"events\", []):\n",
        "        nested = event.get(\"events\", [])\n",
        "        if isinstance(nested, list):\n",
        "            count += len(nested)\n",
        "    return count\n",
        "\n",
        "def hypothesis_count(task):\n",
        "    \"\"\"\n",
        "    Counts the number of top-level Hypothesis events in a task.\n",
        "    \"\"\"\n",
        "    return sum(1 for event in task.get(\"events\", []) if event.get(\"event_class\") == \"Hypothesis\")\n"
      ],
      "metadata": {
        "id": "MC49NQ4SX3SG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute and plot basic metrics\n",
        "\n",
        "basic_metrics = {\n",
        "    #\"task_count\": lambda task: 1,\n",
        "    #\"event_count\": second_order_event_count,\n",
        "    \"average_hypothesis_complexity\": average_hypothesis_complexity,\n",
        "    #\"hypothesis_count\": hypothesis_count,\n",
        "}\n",
        "\n",
        "basic_metrics_df = compute_all_task_metrics(all_tasks, basic_metrics)\n",
        "\n",
        "aggfuncs = {\n",
        "    \"tool_task\": \"sum\",  #\n",
        "    \"per_task\": \"var\" # var, sum, mean\n",
        "}\n",
        "\n",
        "summary = aggregate_precomputed_task_metrics(basic_metrics_df, aggfuncs)\n",
        "summary = format_summary_columns(summary)\n",
        "\n",
        "#print(\"ðŸ“Š Participant-Task Summary\")\n",
        "#display(summary[\"participant_task\"])\n",
        "\n",
        "print(\"ðŸ“Š Tool-Task Summary\")\n",
        "display(summary[\"tool_task\"])\n",
        "\n",
        "print(\"ðŸ“Š Mean per Task\")\n",
        "display(summary[\"per_task\"])\n",
        "\n",
        "print(\"ðŸ“Š Mean per Participant\")\n",
        "display(summary[\"per_participant\"])\n",
        "\n",
        "print(\"ðŸ“Š Mean per Tool\")\n",
        "display(summary[\"per_tool\"])\n",
        "\n",
        "\n",
        "metrics_list=[\"average_hypothesis_complexity\"]\n",
        "\n",
        "one_way_anova_df = perform_one_way_anova(df=basic_metrics_df, metrics=metrics_list, factor=\"tool_name\")\n",
        "print(\"One way anova per tool\")\n",
        "display(one_way_anova_df)\n",
        "two_way_anova_tasks = perform_two_way_anova(df=basic_metrics_df, metrics=metrics_list, factors=[\"tool_name\", \"task_number\"])\n",
        "print(\"Two way anova per tool and task\")\n",
        "display(two_way_anova_tasks)\n",
        "two_way_anova_participants = perform_two_way_anova(df=basic_metrics_df, metrics=metrics_list, factors=[\"tool_name\", \"participant_id\"])\n",
        "print(\"Two way anova per tool and paticipant\")\n",
        "display(two_way_anova_participants)"
      ],
      "metadata": {
        "id": "Dlg_qjFtabf6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Confirmation metrics"
      ],
      "metadata": {
        "id": "AnFxbJOzaJG7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#################################\n",
        "#  CONFIRMATION BIAS FUNCTIONS  #\n",
        "#################################\n",
        "\n",
        "def confirmation_action_ratio(task):\n",
        "    \"\"\"\n",
        "    Computes the ratio of confirming actions over total (confirm + disprove)\n",
        "    from second-order Action events inside top-level Hypothesis events.\n",
        "    \"\"\"\n",
        "    confirm = 0\n",
        "    disprove = 0\n",
        "\n",
        "    for event in task.get(\"events\", []):\n",
        "        # Only search inside Hypothesis-type top-level events\n",
        "        if event.get(\"event_class\") == \"Hypothesis\":\n",
        "            for subevent in event.get(\"events\", []):\n",
        "                if subevent.get(\"event_class\") == \"Action\":\n",
        "                    goal = subevent.get(\"goal\", \"\").lower()\n",
        "                    if goal == \"confirm\":\n",
        "                        confirm += 1\n",
        "                    elif goal == \"disprove\":\n",
        "                        disprove += 1\n",
        "\n",
        "    total = confirm + disprove\n",
        "    return confirm / total if total > 0 else None\n",
        "\n",
        "def disconfirmation_action_ratio(task):\n",
        "    confirm = 0\n",
        "    disprove = 0\n",
        "\n",
        "    for event in task.get(\"events\", []):\n",
        "        if event.get(\"event_class\") == \"Hypothesis\":\n",
        "            for subevent in event.get(\"events\", []):\n",
        "                if subevent.get(\"event_class\") == \"Action\":\n",
        "                    goal = subevent.get(\"goal\", \"\").lower()\n",
        "                    if goal == \"confirm\":\n",
        "                        confirm += 1\n",
        "                    elif goal == \"disprove\":\n",
        "                        disprove += 1\n",
        "\n",
        "    total = confirm + disprove\n",
        "    return disprove / total if total > 0 else None\n",
        "\n",
        "\n",
        "def hypothesis_confirmation_score_strict(task):\n",
        "    \"\"\"\n",
        "    Returns strict confirmation score:\n",
        "    confirmed / total evaluated hypotheses\n",
        "    \"\"\"\n",
        "    confirmed = 0\n",
        "    evaluated = 0\n",
        "\n",
        "    for event in task.get(\"events\", []):\n",
        "        if event.get(\"event_class\") == \"Hypothesis\":\n",
        "            evaluation = event.get(\"evaluation\", \"\").lower()\n",
        "            if evaluation in {\"confirmed\", \"weakly_confirmed\", \"disproved\", \"weakly_disproved\"}:\n",
        "                evaluated += 1\n",
        "                if evaluation == \"confirmed\":\n",
        "                    confirmed += 1\n",
        "\n",
        "    return confirmed / evaluated if evaluated > 0 else None\n",
        "\n",
        "\n",
        "def hypothesis_confirmation_score_lenient(task):\n",
        "    \"\"\"\n",
        "    Returns lenient confirmation score:\n",
        "    (confirmed + weakly_confirmed) / total evaluated hypotheses\n",
        "    \"\"\"\n",
        "    confirmed = 0\n",
        "    evaluated = 0\n",
        "\n",
        "    for event in task.get(\"events\", []):\n",
        "        if event.get(\"event_class\") == \"Hypothesis\":\n",
        "            evaluation = event.get(\"evaluation\", \"\").lower()\n",
        "            if evaluation in {\"confirmed\", \"weakly_confirmed\", \"disproved\", \"weakly_disproved\"}:\n",
        "                evaluated += 1\n",
        "                if evaluation in {\"confirmed\", \"weakly_confirmed\"}:\n",
        "                    confirmed += 1\n",
        "\n",
        "    return confirmed / evaluated if evaluated > 0 else None\n",
        "# measuring inconsistency in hypothesis evaluation\n",
        "\n",
        "LABEL_SCORES = {\n",
        "    \"confirmed\": 0.0,\n",
        "    \"weakly_confirmed\": 0.25,\n",
        "    \"partially_confirmed_or_disproved\": 0.5,\n",
        "    \"weakly_disproved\": 0.75,\n",
        "    \"disproved\": 1.0\n",
        "}\n",
        "\n",
        "def is_close_to_label_or_midpoint(hyp_score, avg_score, label_scores=LABEL_SCORES, tolerance=0.01):\n",
        "    if abs(hyp_score - avg_score) <= tolerance:\n",
        "        return True  # exact match\n",
        "\n",
        "    # Check midpoint match\n",
        "    sorted_scores = sorted(label_scores.values())\n",
        "    for a, b in zip(sorted_scores, sorted_scores[1:]):\n",
        "        midpoint = round((a + b) / 2, 5)\n",
        "        if abs(avg_score - midpoint) <= tolerance and hyp_score in (a, b):\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "def contradiction_ignoring_index(task):\n",
        "    def get_score(label, goal):\n",
        "        score = LABEL_SCORES.get(label, None)\n",
        "        if score is None:\n",
        "            return None\n",
        "        if goal == \"disprove\":\n",
        "            return 1.0 - score\n",
        "        return score\n",
        "\n",
        "    hypothesis_differences = []\n",
        "\n",
        "    for hyp in task.get(\"events\", []):\n",
        "        if hyp.get(\"event_class\") != \"Hypothesis\":\n",
        "            continue\n",
        "\n",
        "        hyp_eval = hyp.get(\"evaluation\", \"\").lower()\n",
        "        hyp_score = LABEL_SCORES.get(hyp_eval)\n",
        "        if hyp_score is None:\n",
        "            continue\n",
        "\n",
        "        action_scores = []\n",
        "        for act in hyp.get(\"events\", []):\n",
        "            if act.get(\"event_class\") != \"Action\":\n",
        "                continue\n",
        "\n",
        "            label = act.get(\"evaluation\", \"\").lower()\n",
        "            goal = act.get(\"goal\", \"\").lower()\n",
        "            score = get_score(label, goal)\n",
        "            if score is not None:\n",
        "                action_scores.append(score)\n",
        "\n",
        "        if action_scores:\n",
        "            avg_action_score = sum(action_scores) / len(action_scores)\n",
        "            aligned = is_close_to_label_or_midpoint(hyp_score, avg_action_score)\n",
        "            diff = 0.0 if aligned else round(abs(hyp_score - avg_action_score), 2)\n",
        "            hypothesis_differences.append(diff)\n",
        "\n",
        "    if not hypothesis_differences:\n",
        "        return None\n",
        "\n",
        "    return sum(hypothesis_differences) / len(hypothesis_differences)\n",
        "\n",
        "def extract_hypothesis_evaluation_differences(all_tasks):\n",
        "    \"\"\"\n",
        "    For each task and hypothesis, compute:\n",
        "    - participant evaluation score\n",
        "    - mean action score\n",
        "    - absolute difference\n",
        "    Returns a flat DataFrame: one row per hypothesis.\n",
        "    \"\"\"\n",
        "    def get_score(label, goal):\n",
        "        score = LABEL_SCORES.get(label, None)\n",
        "        if score is None:\n",
        "            return None\n",
        "        return 1.0 - score if goal == \"disprove\" else score\n",
        "\n",
        "    rows = []\n",
        "\n",
        "    for task in all_tasks:\n",
        "        pid = task.get(\"participant_id\")\n",
        "        tid = task.get(\"task_number\")\n",
        "        tool = task.get(\"tool_name\")\n",
        "\n",
        "        for i, hyp in enumerate(task.get(\"events\", [])):\n",
        "            if hyp.get(\"event_class\") != \"Hypothesis\":\n",
        "                continue\n",
        "\n",
        "            hyp_eval = hyp.get(\"evaluation\", \"\").lower()\n",
        "            hyp_score = LABEL_SCORES.get(hyp_eval)\n",
        "            if hyp_score is None:\n",
        "                continue\n",
        "\n",
        "            action_scores = []\n",
        "            for act in hyp.get(\"events\", []):\n",
        "                if act.get(\"event_class\") != \"Action\":\n",
        "                    continue\n",
        "                label = act.get(\"evaluation\", \"\").lower()\n",
        "                goal = act.get(\"goal\", \"\").lower()\n",
        "                score = get_score(label, goal)\n",
        "                if score is not None:\n",
        "                    action_scores.append(score)\n",
        "\n",
        "            if not action_scores:\n",
        "                continue\n",
        "\n",
        "            avg_action_score = sum(action_scores) / len(action_scores)\n",
        "            aligned = is_close_to_label_or_midpoint(hyp_score, avg_action_score)\n",
        "            diff = 0.0 if aligned else round(abs(hyp_score - avg_action_score), 2)\n",
        "\n",
        "            rows.append({\n",
        "                \"participant_id\": pid,\n",
        "                \"task_number\": tid,\n",
        "                \"tool_name\": tool,\n",
        "                \"hypothesis_index\": i,\n",
        "                \"hypothesis_eval\": hyp_eval,\n",
        "                \"avg_action_score\": round(avg_action_score, 3),\n",
        "                \"difference\": diff\n",
        "            })\n",
        "\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "def contradictory_evidence_ratio(task):\n",
        "    \"\"\"\n",
        "    Calculates the mean ratio of contradictory actions to total actions\n",
        "    per hypothesis in the task.\n",
        "    \"\"\"\n",
        "    ratios = []\n",
        "\n",
        "    for hyp in task.get(\"events\", []):\n",
        "        if hyp.get(\"event_class\") != \"Hypothesis\":\n",
        "            continue\n",
        "\n",
        "        hyp_eval = hyp.get(\"evaluation\", \"\").lower()\n",
        "        if hyp_eval == \"partially_confirmed_or_disproved\":\n",
        "            continue\n",
        "\n",
        "        is_confirm_family = hyp_eval in {\"confirmed\", \"weakly_confirmed\"}\n",
        "        is_disprove_family = hyp_eval in {\"disproved\", \"weakly_disproved\"}\n",
        "\n",
        "        contradictory_count = 0\n",
        "        action_count = 0\n",
        "\n",
        "        for act in hyp.get(\"events\", []):\n",
        "            if act.get(\"event_class\") != \"Action\":\n",
        "                continue\n",
        "\n",
        "            label = act.get(\"evaluation\", \"\").lower()\n",
        "            if not label or label == \"partially_confirmed_or_disproved\":\n",
        "                continue\n",
        "\n",
        "            action_count += 1\n",
        "            if is_confirm_family and label in {\"disproved\", \"weakly_disproved\"}:\n",
        "                contradictory_count += 1\n",
        "            elif is_disprove_family and label in {\"confirmed\", \"weakly_confirmed\"}:\n",
        "                contradictory_count += 1\n",
        "\n",
        "        if action_count > 0:\n",
        "            ratios.append(contradictory_count / action_count)\n",
        "\n",
        "    return sum(ratios) / len(ratios) if ratios else 0.0\n",
        "\n",
        "\n",
        "\n",
        "def extract_contradictory_evidence(all_tasks):\n",
        "    \"\"\"\n",
        "    Returns a flat DataFrame: one row per hypothesis,\n",
        "    with count of contradictory action labels and description.\n",
        "    \"\"\"\n",
        "    rows = []\n",
        "\n",
        "    for task in all_tasks:\n",
        "        pid = task.get(\"participant_id\")\n",
        "        tid = task.get(\"task_number\")\n",
        "        tool = task.get(\"tool_name\")\n",
        "\n",
        "        for i, hyp in enumerate(task.get(\"events\", [])):\n",
        "            if hyp.get(\"event_class\") != \"Hypothesis\":\n",
        "                continue\n",
        "\n",
        "            hyp_eval = hyp.get(\"evaluation\", \"\").lower()\n",
        "            description = hyp.get(\"description\", \"\").strip()\n",
        "\n",
        "            if hyp_eval == \"partially_confirmed_or_disproved\":\n",
        "                contradictory_count = 0\n",
        "            else:\n",
        "                is_confirm_family = hyp_eval in {\"confirmed\", \"weakly_confirmed\"}\n",
        "                is_disprove_family = hyp_eval in {\"disproved\", \"weakly_disproved\"}\n",
        "\n",
        "                contradictory_count = 0\n",
        "                for act in hyp.get(\"events\", []):\n",
        "                    if act.get(\"event_class\") != \"Action\":\n",
        "                        continue\n",
        "                    label = act.get(\"evaluation\", \"\").lower()\n",
        "                    if not label or label == \"partially_confirmed_or_disproved\":\n",
        "                        continue\n",
        "\n",
        "                    if is_confirm_family and label in {\"disproved\", \"weakly_disproved\"}:\n",
        "                        contradictory_count += 1\n",
        "                    elif is_disprove_family and label in {\"confirmed\", \"weakly_confirmed\"}:\n",
        "                        contradictory_count += 1\n",
        "\n",
        "            rows.append({\n",
        "                \"participant_id\": pid,\n",
        "                \"task_number\": tid,\n",
        "                \"tool_name\": tool,\n",
        "                \"hypothesis_index\": i,\n",
        "                \"hypothesis_eval\": hyp_eval,\n",
        "                \"hypothesis_description\": description,\n",
        "                \"contradictory_count\": contradictory_count\n",
        "            })\n",
        "\n",
        "    import pandas as pd\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "def goal_strategy_entropy(task):\n",
        "    goals = []\n",
        "    for hyp in task.get(\"events\", []):\n",
        "        if hyp.get(\"event_class\") != \"Hypothesis\":\n",
        "            continue\n",
        "        for act in hyp.get(\"events\", []):\n",
        "            if act.get(\"event_class\") == \"Action\":\n",
        "                goal = act.get(\"goal\", \"\").lower()\n",
        "                if goal:\n",
        "                    goals.append(goal)\n",
        "    return compute_entropy(goals)\n",
        "\n",
        "def class_strategy_entropy(task):\n",
        "    strategies = []\n",
        "    for hyp in task.get(\"events\", []):\n",
        "        if hyp.get(\"event_class\") != \"Hypothesis\":\n",
        "            continue\n",
        "        for act in hyp.get(\"events\", []):\n",
        "            if act.get(\"event_class\") == \"Action\":\n",
        "                cls = act.get(\"class_strategy\") or act.get(\"class_strategy_type\")\n",
        "                if cls:\n",
        "                    strategies.append(cls.lower())\n",
        "    return compute_entropy(strategies)\n",
        "\n",
        "\n",
        "def hypothesis_evaluation_entropy(task):\n",
        "    \"\"\"\n",
        "    Computes entropy over the types of hypothesis evaluations given in the task.\n",
        "    Measures how diversely the participant judged their hypotheses (e.g., confirmed, disproved).\n",
        "    \"\"\"\n",
        "    evals = []\n",
        "    for hyp in task.get(\"events\", []):\n",
        "        if hyp.get(\"event_class\") == \"Hypothesis\":\n",
        "            label = hyp.get(\"evaluation\", \"\").lower()\n",
        "            if label:\n",
        "                evals.append(label)\n",
        "    return compute_entropy(evals)\n",
        "\n",
        "\n",
        "def confirmed_hypothesis_action_agreement_ratio(task):\n",
        "    \"\"\"\n",
        "    For hypotheses labeled as confirmed or weakly confirmed by the participant,\n",
        "    compute how many are also confirmed/weakly_confirmed based on aggregated action evaluations.\n",
        "    Returns: ratio of consistent confirmations.\n",
        "    \"\"\"\n",
        "    def get_score(label, goal):\n",
        "        score = LABEL_SCORES.get(label)\n",
        "        if score is None:\n",
        "            return None\n",
        "        return 1.0 - score if goal == \"disprove\" else score\n",
        "\n",
        "    consistent = 0\n",
        "    total = 0\n",
        "\n",
        "    for hyp in task.get(\"events\", []):\n",
        "        if hyp.get(\"event_class\") != \"Hypothesis\":\n",
        "            continue\n",
        "\n",
        "        hyp_eval = hyp.get(\"evaluation\", \"\").lower()\n",
        "        if hyp_eval not in {\"confirmed\", \"weakly_confirmed\"}:\n",
        "            continue\n",
        "\n",
        "        total += 1\n",
        "\n",
        "        action_scores = []\n",
        "        for act in hyp.get(\"events\", []):\n",
        "            if act.get(\"event_class\") != \"Action\":\n",
        "                continue\n",
        "            label = act.get(\"evaluation\", \"\").lower()\n",
        "            goal = act.get(\"goal\", \"\").lower()\n",
        "            score = get_score(label, goal)\n",
        "            if score is not None:\n",
        "                action_scores.append(score)\n",
        "\n",
        "        if not action_scores:\n",
        "            continue\n",
        "\n",
        "        avg_score = sum(action_scores) / len(action_scores)\n",
        "\n",
        "        # Define thresholds for confirmation family\n",
        "        if avg_score < 0.5:  # midpoint between 0.25 (weakly_confirmed) and 0.5\n",
        "            consistent += 1\n",
        "\n",
        "    return consistent / total if total > 0 else None\n",
        "\n",
        "def disproved_hypothesis_action_agreement_ratio(task):\n",
        "    \"\"\"\n",
        "    For hypotheses labeled as disproved or weakly disproved by the participant,\n",
        "    compute how many are also disproved/weakly_disproved based on aggregated action evaluations.\n",
        "    Returns: ratio of consistent disconfirmations.\n",
        "    \"\"\"\n",
        "    def get_score(label, goal):\n",
        "        score = LABEL_SCORES.get(label)\n",
        "        if score is None:\n",
        "            return None\n",
        "        return 1.0 - score if goal == \"disprove\" else score\n",
        "\n",
        "    consistent = 0\n",
        "    total = 0\n",
        "\n",
        "    for hyp in task.get(\"events\", []):\n",
        "        if hyp.get(\"event_class\") != \"Hypothesis\":\n",
        "            continue\n",
        "\n",
        "        hyp_eval = hyp.get(\"evaluation\", \"\").lower()\n",
        "        if hyp_eval not in {\"disproved\", \"weakly_disproved\"}:\n",
        "            continue\n",
        "\n",
        "        total += 1\n",
        "\n",
        "        action_scores = []\n",
        "        for act in hyp.get(\"events\", []):\n",
        "            if act.get(\"event_class\") != \"Action\":\n",
        "                continue\n",
        "            label = act.get(\"evaluation\", \"\").lower()\n",
        "            goal = act.get(\"goal\", \"\").lower()\n",
        "            score = get_score(label, goal)\n",
        "            if score is not None:\n",
        "                action_scores.append(score)\n",
        "\n",
        "        if not action_scores:\n",
        "            continue\n",
        "\n",
        "        avg_score = sum(action_scores) / len(action_scores)\n",
        "\n",
        "        # Define thresholds for disconfirmation family\n",
        "        if avg_score > 0.5:  # midpoint between 0.5 and 0.75 (weakly_disproved)\n",
        "            consistent += 1\n",
        "\n",
        "    return consistent / total if total > 0 else None\n",
        "\n"
      ],
      "metadata": {
        "id": "klp_Zcnkcw2A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_metrics = {\n",
        "    #\"hypothesis_count\": hypothesis_count,\n",
        "    #\"confirmation_ratio\": confirmation_action_ratio,\n",
        "    #\"disconfirmation_ratio\": disconfirmation_action_ratio,\n",
        "    #\"hypo_conf_score_strict\": hypothesis_confirmation_score_strict,\n",
        "    #\"hypo_conf_score_lenient\": hypothesis_confirmation_score_lenient,\n",
        "    #\"hypothesis_evaluation_entropy\": hypothesis_evaluation_entropy\n",
        "    \"contradiction_ignoring_index\": contradiction_ignoring_index,\n",
        "    \"contradictory_evidence_ratio\": contradictory_evidence_ratio,\n",
        "    \"confirmed_hypothesis_action_agreement_ratio\": confirmed_hypothesis_action_agreement_ratio,\n",
        "    \"disproved_hypothesis_action_agreement_ratio\": disproved_hypothesis_action_agreement_ratio,\n",
        "    \"confirmation_strategy_entropy\": goal_strategy_entropy,\n",
        "}\n",
        "all_metrics_df = compute_all_task_metrics(all_tasks, all_metrics)\n",
        "\n",
        "summary = aggregate_precomputed_task_metrics(all_metrics_df)\n",
        "# Format numeric outputs before display\n",
        "summary = format_summary_columns(summary)\n",
        "print(\"ðŸ“Š Tool-Task Summary\")\n",
        "tool_task_df = summary[\"tool_task\"]\n",
        "tool_task_df = tool_task_df[['task_number', 'CII_wit', 'CII_HX', 'CSE_wit', 'CSE_HX', 'CHAAR_wit', 'CHAAR_HX',\n",
        "        'DHAAR_wit', 'DHAAR_HX', 'CER_wit', 'CER_HX']]\n",
        "display(tool_task_df)\n",
        "\n",
        "print(\"ðŸ“Š Mean per Participant\")\n",
        "participant_df = summary[\"per_participant\"][['participant_id', 'CII_wit', 'CII_HX', 'CSE_wit', 'CSE_HX', 'CHAAR_wit', 'CHAAR_HX',\n",
        "        'DHAAR_wit', 'DHAAR_HX', 'CER_wit', 'CER_HX']]\n",
        "display(participant_df)\n",
        "\n",
        "print(\"ðŸ“Š Mean per Tool\")\n",
        "tool_df = summary[\"per_tool\"][['tool_name', 'CII', 'CER', 'CHAAR', \t'DHAAR', 'CSE']]\n",
        "display(tool_df)\n",
        "\n",
        "metrics_list=[\"contradiction_ignoring_index\", \"contradictory_evidence_ratio\", \"confirmed_hypothesis_action_agreement_ratio\", \"disproved_hypothesis_action_agreement_ratio\", \"confirmation_strategy_entropy\"]\n",
        "one_way_anova_df = perform_one_way_anova(df=all_metrics_df, metrics=metrics_list, factor=\"tool_name\")\n",
        "print(\"One way anova per tool\")\n",
        "display(one_way_anova_df)\n",
        "two_way_anova_tasks = perform_two_way_anova(df=all_metrics_df, metrics=metrics_list, factors=[\"tool_name\", \"task_number\"])\n",
        "print(\"Two way anova per tool and task\")\n",
        "display(two_way_anova_tasks)\n",
        "two_way_anova_participants = perform_two_way_anova(df=all_metrics_df, metrics=metrics_list, factors=[\"tool_name\", \"participant_id\"])\n",
        "print(\"Two way anova per tool and paticipant\")\n",
        "display(two_way_anova_participants)"
      ],
      "metadata": {
        "id": "wsDEX7NOJlyl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = extract_hypothesis_evaluation_differences(all_tasks)\n",
        "#df = extract_contradictory_evidence(all_tasks)\n",
        "display(df)"
      ],
      "metadata": {
        "id": "IDxEJdmLpttX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Anchoring metrics"
      ],
      "metadata": {
        "id": "Nap5u9NAaaby"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#################################\n",
        "#  ANCHORING BIAS FUNCTIONS     #\n",
        "#################################\n",
        "\n",
        "def strategy_entropy(task):\n",
        "    \"\"\"\n",
        "    Computes strategy entropy for selection_strategy and class_strategy across all Actions.\n",
        "    Returns a dict with two values: selection_entropy and class_entropy\n",
        "    \"\"\"\n",
        "    selection_strategies = []\n",
        "    class_strategies = []\n",
        "\n",
        "    for hyp in task.get(\"events\", []):\n",
        "        if hyp.get(\"event_class\") != \"Hypothesis\":\n",
        "            continue\n",
        "        for act in hyp.get(\"events\", []):\n",
        "            if act.get(\"event_class\") != \"Action\":\n",
        "                continue\n",
        "\n",
        "            sel = act.get(\"selection_strategy\") or act.get(\"selectionStrategy\")\n",
        "            cls = act.get(\"class_strategy\") or act.get(\"class_strategy_type\")\n",
        "\n",
        "            if sel:\n",
        "                selection_strategies.append(sel)\n",
        "            if cls:\n",
        "                class_strategies.append(cls)\n",
        "\n",
        "    return {\n",
        "        \"selection_strategy_entropy\": compute_entropy(selection_strategies),\n",
        "        \"class_strategy_entropy\": compute_entropy(class_strategies)\n",
        "    }\n",
        "\n",
        "def strategy_entropy_selection(task):\n",
        "    return strategy_entropy(task).get(\"selection_strategy_entropy\")\n",
        "\n",
        "def strategy_entropy_class(task):\n",
        "    return strategy_entropy(task).get(\"class_strategy_entropy\")\n",
        "\n",
        "def hypothesis_feature_coverage(task):\n",
        "    \"\"\"\n",
        "    Returns average feature coverage across all hypotheses:\n",
        "    (# tested features âˆ© declared) / (# declared)\n",
        "    \"\"\"\n",
        "    coverage_scores = []\n",
        "\n",
        "    for hyp in task.get(\"events\", []):\n",
        "        if hyp.get(\"event_class\") != \"Hypothesis\":\n",
        "            continue\n",
        "\n",
        "        declared = set(extract_declared_features(hyp.get(\"description\", \"\")))\n",
        "        if not declared:\n",
        "            continue\n",
        "\n",
        "        tested = set()\n",
        "        for act in hyp.get(\"events\", []):\n",
        "            if act.get(\"event_class\") != \"Action\":\n",
        "                continue\n",
        "            tested.update([f.lower() for f in act.get(\"axis_on\", [])])\n",
        "            for op in act.get(\"operations\", []):\n",
        "                feat_match = re.match(r\"(f\\d+)\\s*=\", op.lower())\n",
        "                if feat_match:\n",
        "                    tested.add(feat_match.group(1))\n",
        "\n",
        "        coverage = len(tested & declared) / len(declared)\n",
        "        coverage_scores.append(coverage)\n",
        "\n",
        "    if not coverage_scores:\n",
        "        return None\n",
        "\n",
        "    return round(sum(coverage_scores) / len(coverage_scores), 3)\n",
        "\n",
        "def feature_test_balance(task):\n",
        "    \"\"\"\n",
        "    Computes the entropy over feature usage (axis_on + operations).\n",
        "    Lower = one-feature focus (anchoring), higher = balanced.\n",
        "    \"\"\"\n",
        "    from collections import Counter\n",
        "    features = []\n",
        "\n",
        "    for hyp in task.get(\"events\", []):\n",
        "        if hyp.get(\"event_class\") != \"Hypothesis\":\n",
        "            continue\n",
        "        for act in hyp.get(\"events\", []):\n",
        "            if act.get(\"event_class\") != \"Action\":\n",
        "                continue\n",
        "            features += [f.lower() for f in act.get(\"axis_on\", [])]\n",
        "            for op in act.get(\"operations\", []):\n",
        "                feat_match = re.match(r\"(f\\d+)\\s*=\", op.lower())\n",
        "                if feat_match:\n",
        "                    features.append(feat_match.group(1))\n",
        "\n",
        "    return compute_entropy(features)\n",
        "\n",
        "def feature_drift_score(task):\n",
        "    \"\"\"\n",
        "    Measures the proportion of features used in actions that were NOT declared in the hypothesis.\n",
        "    \"\"\"\n",
        "    declared = set()\n",
        "    used = set()\n",
        "\n",
        "    for hyp in task.get(\"events\", []):\n",
        "        if hyp.get(\"event_class\") != \"Hypothesis\":\n",
        "            continue\n",
        "        declared.update(extract_declared_features(hyp.get(\"description\", \"\")))\n",
        "\n",
        "        for act in hyp.get(\"events\", []):\n",
        "            if act.get(\"event_class\") != \"Action\":\n",
        "                continue\n",
        "            used.update([f.lower() for f in act.get(\"axis_on\", [])])\n",
        "            for op in act.get(\"operations\", []):\n",
        "                feat_match = re.match(r\"(f\\d+)\\s*=\", op.lower())\n",
        "                if feat_match:\n",
        "                    used.add(feat_match.group(1))\n",
        "\n",
        "    if not used:\n",
        "        return None\n",
        "\n",
        "    drift = len(used - declared) / len(used)\n",
        "    return round(drift, 3)\n",
        "\n",
        "def main_profile_reached_score(task):\n",
        "    \"\"\"\n",
        "    Returns the fraction of hypotheses in this task that touch the main profile\n",
        "    of the predicted goal class. Returns None if no hypotheses with valid actions.\n",
        "    \"\"\"\n",
        "    from collections import Counter\n",
        "\n",
        "    tool_name = task.get(\"tool_name\")\n",
        "    profile = tool_profiles.get(tool_name)\n",
        "    if profile is None:\n",
        "        return None\n",
        "\n",
        "    main_profiles = profile[\"main_profile\"]\n",
        "    point_to_class = profile[\"point_id_to_class\"]\n",
        "\n",
        "    scores = []\n",
        "\n",
        "    for hyp in task.get(\"events\", []):\n",
        "        if hyp.get(\"event_class\") != \"Hypothesis\":\n",
        "            continue\n",
        "        if not hyp.get(\"events\"):\n",
        "            continue\n",
        "\n",
        "        selected_ids = []\n",
        "        for act in hyp.get(\"events\", []):\n",
        "            if act.get(\"event_class\") == \"Action\":\n",
        "                selected_ids.extend([int(pid) for pid in act.get(\"point_ids\", []) if str(pid).isdigit()])\n",
        "\n",
        "        if not selected_ids:\n",
        "            continue\n",
        "\n",
        "        predicted_classes = [point_to_class.get(pid) for pid in selected_ids if pid in point_to_class]\n",
        "        if not predicted_classes:\n",
        "            continue\n",
        "\n",
        "        goal_class_index = Counter(predicted_classes).most_common(1)[0][0]\n",
        "        main_cluster_points = set(main_profiles.get(goal_class_index, []))\n",
        "        selected_set = set(selected_ids)\n",
        "\n",
        "        match = not main_cluster_points.isdisjoint(selected_set)\n",
        "        scores.append(1.0 if match else 0.0)\n",
        "\n",
        "    return sum(scores) / len(scores) if scores else None\n",
        "\n",
        "def subgroup_coverage_score(task):\n",
        "    \"\"\"\n",
        "    For each hypothesis, computes how many predicted class subclusters (KMeans)\n",
        "    were touched by selected points, relative to the total clusters for that class.\n",
        "\n",
        "    Returns:\n",
        "    - Average subgroup coverage across hypotheses (0.0â€“1.0), or None if no valid hypotheses\n",
        "    \"\"\"\n",
        "    from collections import Counter\n",
        "\n",
        "    tool_name = task.get(\"tool_name\")\n",
        "    profile = tool_profiles.get(tool_name)\n",
        "    if profile is None:\n",
        "        return None\n",
        "\n",
        "    clusters_by_class = profile[\"clusters\"]\n",
        "    point_to_class = profile[\"point_id_to_class\"]\n",
        "\n",
        "    scores = []\n",
        "\n",
        "    for hyp in task.get(\"events\", []):\n",
        "        if hyp.get(\"event_class\") != \"Hypothesis\":\n",
        "            continue\n",
        "        if not hyp.get(\"events\"):\n",
        "            continue\n",
        "\n",
        "        selected_ids = []\n",
        "        for act in hyp.get(\"events\", []):\n",
        "            if act.get(\"event_class\") == \"Action\":\n",
        "                selected_ids.extend([int(pid) for pid in act.get(\"point_ids\", []) if str(pid).isdigit()])\n",
        "\n",
        "        if not selected_ids:\n",
        "            continue\n",
        "\n",
        "        predicted_classes = [point_to_class.get(pid) for pid in selected_ids if pid in point_to_class]\n",
        "        if not predicted_classes:\n",
        "            continue\n",
        "\n",
        "        goal_class_index = Counter(predicted_classes).most_common(1)[0][0]\n",
        "        cluster_map = clusters_by_class.get(goal_class_index)\n",
        "        if not cluster_map:\n",
        "            continue\n",
        "\n",
        "        visited_clusters = {cluster_map[pid] for pid in selected_ids if pid in cluster_map}\n",
        "        total_clusters = len(set(cluster_map.values()))\n",
        "        if total_clusters == 0:\n",
        "            continue\n",
        "\n",
        "        coverage = len(visited_clusters) / total_clusters\n",
        "        scores.append(coverage)\n",
        "\n",
        "    return sum(scores) / len(scores) if scores else None\n",
        "\n",
        "def multiclass_selection_entropy(task):\n",
        "    \"\"\"\n",
        "    Calculates the entropy of predicted class distribution for selected points\n",
        "    in all hypothesis actions. Measures how diversely participants explored classes.\n",
        "\n",
        "    Returns:\n",
        "    - Entropy score (0.0 = all points from one class, high = diverse class selection)\n",
        "    - None if no points selected\n",
        "    \"\"\"\n",
        "    tool_name = task.get(\"tool_name\")\n",
        "    profile = tool_profiles.get(tool_name)\n",
        "    if profile is None:\n",
        "        return None\n",
        "\n",
        "    point_to_class = profile[\"point_id_to_class\"]\n",
        "    selected_classes = []\n",
        "\n",
        "    for hyp in task.get(\"events\", []):\n",
        "        if hyp.get(\"event_class\") != \"Hypothesis\":\n",
        "            continue\n",
        "        for act in hyp.get(\"events\", []):\n",
        "            if act.get(\"event_class\") == \"Action\":\n",
        "                int_pids = [int(pid) for pid in act.get(\"point_ids\", []) if str(pid).isdigit()]\n",
        "                selected_classes.extend([\n",
        "                    point_to_class[pid] for pid in int_pids if pid in point_to_class\n",
        "                ])\n",
        "\n",
        "\n",
        "    if not selected_classes:\n",
        "        return None\n",
        "\n",
        "    class_counts = Counter(selected_classes)\n",
        "    probs = np.array(list(class_counts.values())) / len(selected_classes)\n",
        "    return float(entropy(probs, base=2))  # base 2 entropy\n",
        "\n",
        "def goal_class_feature_range_coverage(task):\n",
        "    \"\"\"\n",
        "    For each hypothesis in a task, compute how well the selected test points\n",
        "    cover the feature range of the goal class.\n",
        "\n",
        "    Returns the average per hypothesis, or None if no valid hypotheses.\n",
        "    \"\"\"\n",
        "    tool_name = task.get(\"tool_name\")\n",
        "    profile = tool_profiles.get(tool_name)\n",
        "    if not profile:\n",
        "        return None\n",
        "\n",
        "    label_map = profile.get(\"label_conversion_map\", {})\n",
        "    point_to_features = profile.get(\"point_id_to_features\", {})\n",
        "    class_feature_space = profile.get(\"class_feature_spaces\", {})\n",
        "\n",
        "    hypothesis_scores = []\n",
        "\n",
        "    for hyp in task.get(\"events\", []):\n",
        "        if hyp.get(\"event_class\") != \"Hypothesis\":\n",
        "            continue\n",
        "        goal_class = None\n",
        "        for act in hyp.get(\"events\", []):\n",
        "            if act.get(\"event_class\") == \"Action\":\n",
        "                goal_class = act.get(\"goal_class\", \"\").strip().lower()\n",
        "                break  # only take the first action\n",
        "\n",
        "        goal_idx = label_map.get(goal_class)\n",
        "        if goal_idx is None or goal_idx not in class_feature_space:\n",
        "            continue\n",
        "\n",
        "        # Feature range for the goal class\n",
        "        feature_ranges = {}\n",
        "        for feature_dict in class_feature_space[goal_idx]:\n",
        "            for feat, val in feature_dict.items():\n",
        "                feature_ranges.setdefault(feat, []).append(val)\n",
        "        feature_min = {f: np.min(v) for f, v in feature_ranges.items()}\n",
        "        feature_max = {f: np.max(v) for f, v in feature_ranges.items()}\n",
        "\n",
        "        # Selected points used in the hypothesis\n",
        "        selected_points = []\n",
        "        for act in hyp.get(\"events\", []):\n",
        "            #print(act)\n",
        "            if act.get(\"event_class\") == \"Action\":\n",
        "                point_ids = [int(pid) for pid in act.get(\"point_ids\", []) if str(pid).isdigit()]\n",
        "                selected_points.extend(point_ids)\n",
        "        if not selected_points:\n",
        "            continue\n",
        "\n",
        "        # Compute coverage per feature\n",
        "        coverage_ratios = []\n",
        "        for feat in feature_min.keys():\n",
        "            goal_min, goal_max = feature_min[feat], feature_max[feat]\n",
        "            if goal_min == goal_max:\n",
        "                continue  # Skip constant features\n",
        "            values = [point_to_features[pid][feat] for pid in selected_points if feat in point_to_features[pid]]\n",
        "            if not values:\n",
        "                continue\n",
        "            covered_min = min(values)\n",
        "            covered_max = max(values)\n",
        "\n",
        "            covered_range = max(0, min(goal_max, covered_max) - max(goal_min, covered_min))\n",
        "            full_range = goal_max - goal_min\n",
        "            ratio = covered_range / full_range if full_range > 0 else 0\n",
        "            coverage_ratios.append(ratio)\n",
        "        if coverage_ratios:\n",
        "            hypothesis_scores.append(np.mean(coverage_ratios))\n",
        "\n",
        "    if not hypothesis_scores:\n",
        "        return None\n",
        "    return round(np.mean(hypothesis_scores), 3)\n"
      ],
      "metadata": {
        "id": "lkxJXEY0_Myn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "anchoring_metrics = {\n",
        "    \"main_profile_reached\": main_profile_reached_score,\n",
        "    \"subgroup_coverage\": subgroup_coverage_score,\n",
        "    \"class_strategy_entropy\": strategy_entropy_class,\n",
        "    \"multiclass_selection_entropy\": multiclass_selection_entropy,\n",
        "    \"goal_class_feature_range_coverage\": goal_class_feature_range_coverage,\n",
        "    \"feature_test_balance\": feature_test_balance,\n",
        "    #\"feature_drift_score\": feature_drift_score,\n",
        "    #\"feature_coverage\": hypothesis_feature_coverage,\n",
        "    #\"selection_entropy\": strategy_entropy_selection,\n",
        "}\n",
        "df = compute_all_task_metrics(all_tasks, anchoring_metrics)\n",
        "summary = aggregate_precomputed_task_metrics(df)\n",
        "formatted = format_summary_columns(summary)\n",
        "\n",
        "print(\"ðŸ“Š Tool-Task Summary\")\n",
        "tool_task_df = formatted[\"tool_task\"]\n",
        "tool_task_df = tool_task_df[['task_number', 'MPR_wit', 'MPR_HX', 'SubC_wit',\n",
        "       'SubC_HX', 'CSE_wit', 'CSE_HX', 'MSelE_wit', 'MSelE_HX', 'FC_wit',\n",
        "       'FC_HX', 'FTB_wit', 'FTB_HX']]\n",
        "print(tool_task_df.columns)\n",
        "display(tool_task_df)\n",
        "\n",
        "print(\"ðŸ“Š Mean per Participant\")\n",
        "participant_df = formatted[\"per_participant\"]\n",
        "participant_df = participant_df[['participant_id', 'MPR_wit', 'MPR_HX', 'SubC_wit',\n",
        "       'SubC_HX', 'CSE_wit', 'CSE_HX', 'MSelE_wit', 'MSelE_HX', 'FC_wit',\n",
        "       'FC_HX', 'FTB_wit', 'FTB_HX']]\n",
        "print(participant_df.columns)\n",
        "display(participant_df)\n",
        "\n",
        "print(\"ðŸ“Š Mean per Tool\")\n",
        "tool_df = formatted[\"per_tool\"]\n",
        "tool_df = tool_df[['tool_name', 'MPR', 'SubC', 'CSE', 'MSelE', 'FC', 'FTB']]\n",
        "print(tool_df.columns)\n",
        "display(tool_df)\n",
        "\n",
        "metrics_list=[\"selection_entropy\", \"class_strategy_entropy\", \"feature_coverage\", \"feature_test_balance\", \"feature_drift_score\", \"main_profile_reached\", \"subgroup_coverage\", \"multiclass_selection_entropy\", \"goal_class_feature_range_coverage\"]\n",
        "\n",
        "one_way_anova_df = perform_one_way_anova(df=df, metrics=metrics_list, factor=\"tool_name\")\n",
        "print(\"One way anova per tool\")\n",
        "display(one_way_anova_df)\n",
        "two_way_anova_tasks = perform_two_way_anova(df=df, metrics=metrics_list, factors=[\"tool_name\", \"task_number\"])\n",
        "print(\"Two way anova per tool and task\")\n",
        "display(two_way_anova_tasks)\n",
        "two_way_anova_participants = perform_two_way_anova(df=df, metrics=metrics_list, factors=[\"tool_name\", \"participant_id\"])\n",
        "print(\"Two way anova per tool and paticipant\")\n",
        "display(two_way_anova_participants)\n"
      ],
      "metadata": {
        "id": "7Siy3mR1_7YN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Availability metrics"
      ],
      "metadata": {
        "id": "CCLNHuVpaizG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#################################\n",
        "#  AVAILABILITY BIAS FUNCTIONS  #\n",
        "#################################\n",
        "def early_evaluation_consistency(task):\n",
        "    def get_score(label, goal):\n",
        "        score = LABEL_SCORES.get(label.lower())\n",
        "        if score is None:\n",
        "            return None\n",
        "        return 1.0 - score if goal == \"disprove\" else score\n",
        "\n",
        "    scores = []\n",
        "\n",
        "    for hyp in task.get(\"events\", []):\n",
        "        if hyp.get(\"event_class\") != \"Hypothesis\":\n",
        "            continue\n",
        "\n",
        "        hyp_eval = hyp.get(\"evaluation\", \"\").lower()\n",
        "        hyp_score = LABEL_SCORES.get(hyp_eval)\n",
        "        if hyp_score is None:\n",
        "            continue\n",
        "\n",
        "        actions = [a for a in hyp.get(\"events\", []) if a.get(\"event_class\") == \"Action\"]\n",
        "        if len(actions) < 2:\n",
        "            continue\n",
        "\n",
        "        cutoff = max(1, int(0.3 * len(actions)))\n",
        "        early_actions = actions[:cutoff]\n",
        "\n",
        "        early_scores = [\n",
        "            get_score(a.get(\"evaluation\", \"\"), a.get(\"goal\", \"\"))\n",
        "            for a in early_actions\n",
        "        ]\n",
        "        early_scores = [s for s in early_scores if s is not None]\n",
        "\n",
        "        if not early_scores:\n",
        "            continue\n",
        "\n",
        "        avg_early = sum(early_scores) / len(early_scores)\n",
        "        scores.append(1 - abs(hyp_score - avg_early))\n",
        "\n",
        "    return round(sum(scores) / len(scores), 3) if scores else None\n",
        "\n",
        "def late_evaluation_influence(task):\n",
        "    def get_score(label, goal):\n",
        "        score = LABEL_SCORES.get(label.lower())\n",
        "        if score is None:\n",
        "            return None\n",
        "        return 1.0 - score if goal == \"disprove\" else score\n",
        "\n",
        "    scores = []\n",
        "\n",
        "    for hyp in task.get(\"events\", []):\n",
        "        if hyp.get(\"event_class\") != \"Hypothesis\":\n",
        "            continue\n",
        "\n",
        "        hyp_eval = hyp.get(\"evaluation\", \"\").lower()\n",
        "        hyp_score = LABEL_SCORES.get(hyp_eval)\n",
        "        if hyp_score is None:\n",
        "            continue\n",
        "\n",
        "        actions = [a for a in hyp.get(\"events\", []) if a.get(\"event_class\") == \"Action\"]\n",
        "        if len(actions) < 2:\n",
        "            continue  # Skip if not enough actions for a late phase\n",
        "\n",
        "        cutoff = int(0.8 * len(actions))\n",
        "        if cutoff >= len(actions):\n",
        "            cutoff = len(actions) - 1  # Ensure at least 1 late action\n",
        "\n",
        "        late_actions = actions[cutoff:]\n",
        "        late_scores = [\n",
        "            get_score(a.get(\"evaluation\", \"\"), a.get(\"goal\", \"\"))\n",
        "            for a in late_actions\n",
        "        ]\n",
        "        late_scores = [s for s in late_scores if s is not None]\n",
        "\n",
        "        if not late_scores:\n",
        "            continue\n",
        "\n",
        "        avg_late = sum(late_scores) / len(late_scores)\n",
        "        scores.append(1 - abs(hyp_score - avg_late))\n",
        "\n",
        "    return round(sum(scores) / len(scores), 3) if scores else None\n",
        "\n",
        "def feature_revisiting_rate(task):\n",
        "    from collections import defaultdict\n",
        "\n",
        "    feature_timestamps = defaultdict(list)\n",
        "\n",
        "    for event in task.get(\"events\", []):\n",
        "        if event.get(\"event_class\") == \"Hypothesis\":\n",
        "            for act in event.get(\"events\", []):\n",
        "                if act.get(\"event_class\") == \"Action\":\n",
        "                    ts = act.get(\"created_at\")\n",
        "                    for f in act.get(\"axis_on\", []):\n",
        "                        feature_timestamps[f].append(ts)\n",
        "\n",
        "    revisited = 0\n",
        "    early_features = []\n",
        "\n",
        "    for f, ts_list in feature_timestamps.items():\n",
        "        if len(ts_list) >= 2:\n",
        "            revisited += 1\n",
        "        if ts_list:\n",
        "            early_features.append(f)\n",
        "\n",
        "    return revisited / len(early_features) if early_features else None\n",
        "\n"
      ],
      "metadata": {
        "id": "QII-PktxBYv9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_metrics = {\n",
        "    \"early_evaluation_consistency\": early_evaluation_consistency,\n",
        "    \"late_evaluation_influence\": late_evaluation_influence,\n",
        "    \"average_hypothesis_complexity\": average_hypothesis_complexity,\n",
        "    #\"feature_revisiting_rate\": feature_revisiting_rate,\n",
        "}\n",
        "\n",
        "availability_metrics_df = compute_all_task_metrics(all_tasks, all_metrics)\n",
        "print(availability_metrics_df)\n",
        "\n",
        "summary = aggregate_precomputed_task_metrics(availability_metrics_df)\n",
        "# Format numeric outputs before display\n",
        "summary = format_summary_columns(summary)\n",
        "print(\"ðŸ“Š Tool-Task Summary\")\n",
        "display(summary[\"tool_task\"])\n",
        "\n",
        "print(\"ðŸ“Š Mean per Participant\")\n",
        "display(summary[\"per_participant\"])\n",
        "\n",
        "print(\"ðŸ“Š Mean per Tool\")\n",
        "display(summary[\"per_tool\"])\n",
        "\n",
        "metrics_list=[\"early_evaluation_consistency\", \"late_evaluation_influence\", \"average_hypothesis_complexity\"]\n",
        "one_way_anova_df = perform_one_way_anova(df=availability_metrics_df, metrics=metrics_list, factor=\"tool_name\")\n",
        "print(\"One way anova per tool\")\n",
        "display(one_way_anova_df)\n",
        "two_way_anova_tasks = perform_two_way_anova(df=availability_metrics_df, metrics=metrics_list, factors=[\"tool_name\", \"task_number\"])\n",
        "print(\"Two way anova per tool and task\")\n",
        "display(two_way_anova_tasks)\n",
        "two_way_anova_participants = perform_two_way_anova(df=availability_metrics_df, metrics=metrics_list, factors=[\"tool_name\", \"participant_id\"])\n",
        "print(\"Two way anova per tool and paticipant\")\n",
        "display(two_way_anova_participants)"
      ],
      "metadata": {
        "id": "hBlcMAe6BY_U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Other metrics"
      ],
      "metadata": {
        "id": "ZDB_sWZsanUC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "uploaded = files.upload()\n",
        "file_name = list(uploaded.keys())[0]  # Get the first uploaded file name\n",
        "text_answer_df = pd.read_csv(io.BytesIO(uploaded[file_name]))\n",
        "\n",
        "display(text_answer_df)\n",
        "\n",
        "metrics_list = [\"importance_score\",\t\"value_score\",\t\"profile_score\",\t\"overall_score\"]\n",
        "\n",
        "\n",
        "one_way_anova_df = perform_one_way_anova(df=text_answer_df, metrics=metrics_list, factor=\"participant_id\")\n",
        "print(\"One way anova per participant\")\n",
        "display(one_way_anova_df)\n",
        "two_way_anova_participants = perform_two_way_anova(df=text_answer_df, metrics=metrics_list, factors=[\"tool_name\", \"participant_id\"])\n",
        "print(\"Two way anova per tool and paticipant\")\n",
        "display(two_way_anova_participants)\n"
      ],
      "metadata": {
        "id": "l2qCjUk6Mkcm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for sufficient variation in metrics (more than 1 unique value)\n",
        "def check_metric_variation(df, metrics):\n",
        "    metric_variation = {}\n",
        "    for metric in metrics:\n",
        "        unique_values = df[metric].nunique()\n",
        "        metric_variation[metric] = unique_values\n",
        "        if unique_values < 2:\n",
        "            print(f\"Warning: Metric '{metric}' has insufficient variation (only {unique_values} unique values).\")\n",
        "    return metric_variation\n",
        "\n",
        "# Check for sufficient variation in factors (more than 1 unique value for each factor)\n",
        "def check_factor_variation(df, factors):\n",
        "    factor_variation = {}\n",
        "    for factor in factors:\n",
        "        unique_values = df[factor].nunique()\n",
        "        factor_variation[factor] = unique_values\n",
        "        if unique_values < 2:\n",
        "            print(f\"Warning: Factor '{factor}' has insufficient variation (only {unique_values} unique values).\")\n",
        "    return factor_variation\n",
        "\n",
        "# Define metrics and factors\n",
        "metrics = [\"importance_score\", \"value_score\", \"profile_score\", \"overall_score\"]\n",
        "factors = [\"tool_name\", \"participant_id\"]\n",
        "\n",
        "# Check the variation for metrics and factors\n",
        "metric_variation = check_metric_variation(text_answer_df, metrics)\n",
        "factor_variation = check_factor_variation(text_answer_df, factors)\n",
        "\n",
        "# Print the results of the checks\n",
        "print(\"\\nMetric Variation:\", metric_variation)\n",
        "print(\"\\nFactor Variation:\", factor_variation)\n"
      ],
      "metadata": {
        "id": "WMGIIS-JXjcp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def hypothesis_success_rate(task):\n",
        "    evaluation_weights = {\n",
        "        \"confirmed\": 1.0,\n",
        "        \"weakly_confirmed\": 0.5,\n",
        "        \"disproved\": 0.0,\n",
        "        \"weakly_disproved\": 0.0\n",
        "    }\n",
        "\n",
        "    total_score = 0.0\n",
        "    valid_count = 0\n",
        "\n",
        "    for event in task.get(\"events\", []):\n",
        "        if event.get(\"event_class\") == \"Hypothesis\":\n",
        "            for action in event.get(\"events\", []):\n",
        "                eval_label = action.get(\"evaluation\", \"\").strip().lower()\n",
        "                if eval_label in evaluation_weights:\n",
        "                    total_score += evaluation_weights[eval_label]\n",
        "                    valid_count += 1\n",
        "\n",
        "    return total_score / valid_count if valid_count > 0 else None\n",
        "\n",
        "\n",
        "def task_duration_minutes(task):\n",
        "    return task.get(\"duration\", None)\n",
        "\n",
        "from datetime import datetime\n",
        "\n",
        "def inspect_before_modify_ratio(task):\n",
        "    def parse_time(t): return datetime.fromisoformat(t.replace(\"Z\", \"+00:00\"))\n",
        "\n",
        "    all_events = task.get(\"events\", [])\n",
        "    if not all_events:\n",
        "        return None\n",
        "\n",
        "    # Flatten hypothesis internal events too\n",
        "    flat_events = []\n",
        "    for event in all_events:\n",
        "        flat_events.append(event)\n",
        "        if event.get(\"event_class\") == \"Hypothesis\":\n",
        "            for subevent in event.get(\"events\", []):\n",
        "                subevent[\"_is_action\"] = True\n",
        "                subevent[\"_parent_time\"] = event.get(\"created_at\")\n",
        "                flat_events.append(subevent)\n",
        "\n",
        "    # Sort by timestamp to preserve chronological order\n",
        "    flat_events.sort(key=lambda e: e.get(\"created_at\"))\n",
        "\n",
        "    modifies = 0\n",
        "    with_inspect = 0\n",
        "    prior_inspect_times = []\n",
        "\n",
        "    for event in flat_events:\n",
        "        event_type = event.get(\"type\", \"\") or \"\"\n",
        "        event_class = event.get(\"event_class\", \"\") or \"\"\n",
        "        event_time = parse_time(event[\"created_at\"])\n",
        "\n",
        "        # Record inspect-type explorations\n",
        "        if event_class == \"DataExploration\" and \"inspect\" in event_type.lower():\n",
        "            prior_inspect_times.append(event_time)\n",
        "\n",
        "        # For Modify_Feature_Value, check if any prior inspect exists\n",
        "        elif event.get(\"_is_action\") and event.get(\"type\") == \"Modify_Feature_Value\":\n",
        "            modifies += 1\n",
        "            if any(t < event_time for t in prior_inspect_times):\n",
        "                with_inspect += 1\n",
        "\n",
        "    return with_inspect / modifies if modifies > 0 else None\n",
        "\n",
        "def feature_exploration_counts(task):\n",
        "    all_features = [f\"f{i}\" for i in range(1, 6)]  # Adjust range if you have more\n",
        "    feature_counter = Counter({f: 0 for f in all_features})\n",
        "\n",
        "    for event in task.get(\"events\", []):\n",
        "        if event.get(\"event_class\") == \"DataExploration\" and event.get(\"type\") == \"Change_Scatterplot_Axis\":\n",
        "            for fid in event.get(\"feature_ids\", []):\n",
        "                if fid in feature_counter:\n",
        "                    feature_counter[fid] += 1\n",
        "\n",
        "    # Rename keys to be column-safe\n",
        "    return {f\"{k}_explorations\": v for k, v in feature_counter.items()}\n",
        "\n"
      ],
      "metadata": {
        "id": "GPfl2kdcqSyq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "task_metrics = {\n",
        "    \"hypothesis_success_rate\": hypothesis_success_rate,\n",
        "    \"task_duration_minutes\": task_duration_minutes,\n",
        "    \"inspect_before_modify_ratio\": inspect_before_modify_ratio,\n",
        "    #\"hypothesis_count\": hypothesis_count,\n",
        "    #\"average_hypothesis_complexity\": average_hypothesis_complexity,\n",
        "    #\"feature_exploration_counts\": feature_exploration_counts\n",
        "}\n",
        "\n",
        "print(\"Number of tasks loaded:\", len(all_tasks))\n",
        "metrics_df = compute_all_task_metrics(all_tasks, task_metrics)\n",
        "display(metrics_df)\n",
        "\n",
        "summary = aggregate_precomputed_task_metrics(metrics_df)\n",
        "# Format numeric outputs before display\n",
        "summary = format_summary_columns(summary)\n",
        "print(\"ðŸ“Š Tool-Task Summary\")\n",
        "display(summary[\"tool_task\"])\n",
        "\n",
        "print(\"ðŸ“Š Mean per Participant\")\n",
        "display(summary[\"per_participant\"])\n",
        "\n",
        "print(\"ðŸ“Š Mean per Tool\")\n",
        "display(summary[\"per_tool\"])\n",
        "\n",
        "metrics_list=[\"average_hypothesis_complexity\", \"inspect_before_modify_ratio\", \"task_duration_minutes\"]\n",
        "one_way_anova_df = perform_one_way_anova(df=metrics_df, metrics=metrics_list, factor=\"tool_name\")\n",
        "print(\"One way anova per tool\")\n",
        "display(one_way_anova_df)\n",
        "two_way_anova_tasks = perform_two_way_anova(df=metrics_df, metrics=metrics_list, factors=[\"tool_name\", \"task_number\"])\n",
        "print(\"Two way anova per tool and task\")\n",
        "display(two_way_anova_tasks)\n",
        "two_way_anova_participants = perform_two_way_anova(df=metrics_df, metrics=metrics_list, factors=[\"tool_name\", \"participant_id\"])\n",
        "print(\"Two way anova per tool and paticipant\")\n",
        "display(two_way_anova_participants)"
      ],
      "metadata": {
        "id": "hHXe9atRsPGU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}