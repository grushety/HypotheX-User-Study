{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ§° Setup & Imports"
      ],
      "metadata": {
        "id": "BZ2lcPYESZgA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EalbqxgSZOFH"
      },
      "outputs": [],
      "source": [
        "!pip install shap eli5 scikit-learn tqdm alibi joblib"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import joblib\n",
        "import numpy as np\n",
        "import pickle\n",
        "import shap\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import files\n",
        "from tqdm import tqdm\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.tree import export_text\n",
        "from sklearn.inspection import PartialDependenceDisplay\n",
        "from alibi.explainers import AnchorTabular"
      ],
      "metadata": {
        "id": "RP_VQFIdZRH3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ“‚ Load Data and Artifacts"
      ],
      "metadata": {
        "id": "OdL15UMNSKc_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "uploaded = files.upload()\n",
        "\n",
        "df = pd.read_csv('alien_dataset.csv')  # <--- Update if animals or aliens\n",
        "X = df.drop(columns=['Class'])\n",
        "y = df['Class']\n",
        "\n",
        "\n",
        "# Load model, scaler, label encoder using joblib\n",
        "model = joblib.load('model.pkl')\n",
        "scaler = joblib.load('scaler.pkl')\n",
        "label_encoder = joblib.load('label_encoder.pkl')\n",
        "\n",
        "# Scale features\n",
        "X_scaled = scaler.transform(X)\n",
        "\n",
        "# ðŸ¤– Predict\n",
        "print(\"Predicting classes...\")\n",
        "y_pred = model.predict(X_scaled)\n",
        "\n",
        "feature_names = X.columns\n",
        "feature_colors = {\n",
        "    feature: color for feature, color in zip(\n",
        "        feature_names,\n",
        "        ['red', 'green', 'blue', 'orange', 'purple']\n",
        "    )\n",
        "}"
      ],
      "metadata": {
        "id": "8kcpAGtmSMtS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ“Š SHAP Analysis: Per Class & Cluster"
      ],
      "metadata": {
        "id": "mwHZZ5JfSlmG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------------------\n",
        "# Setup: Define variables\n",
        "# ------------------------------------------\n",
        "\n",
        "n_clusters_list = [2, 3, 4, 5]\n",
        "\n",
        "feature_importances_by_class_cluster_average = {}\n",
        "feature_importances_by_class_average = {}\n",
        "feature_importances_by_class = {}\n",
        "feature_importances_by_class_cluster= {}\n",
        "\n",
        "# ------------------------------------------\n",
        "# Build SHAP Explainer Once\n",
        "# ------------------------------------------\n",
        "\n",
        "print(\"Building SHAP explainer...\")\n",
        "explainer = shap.TreeExplainer(model)\n",
        "\n",
        "# Before loop\n",
        "shap_values = explainer.shap_values(X_scaled)\n",
        "\n",
        "# ------------------------------------------\n",
        "# For each class\n",
        "# ------------------------------------------\n",
        "\n",
        "for class_idx, class_label in enumerate(label_encoder.classes_):\n",
        "    print(f\"\\nAnalyzing class: {class_label}\")\n",
        "\n",
        "    idx_class = np.where(y_pred == class_idx)[0]\n",
        "    X_class = X_scaled[idx_class]\n",
        "\n",
        "    if len(X_class) == 0:\n",
        "        continue\n",
        "\n",
        "    # Extract SHAP values for this class\n",
        "    shap_vals_class = shap_values[idx_class, :, class_idx]  # (samples_in_class, features)\n",
        "\n",
        "    # Save mean(abs(SHAP)) for whole class\n",
        "    mean_abs_shap_class = np.abs(shap_vals_class).mean(axis=0)\n",
        "    feature_importances_by_class_average[class_label] = mean_abs_shap_class\n",
        "    feature_importances_by_class[class_label] = shap_vals_class\n",
        "\n",
        "    # ------------------------------------------\n",
        "    # Now cluster inside class\n",
        "    # ------------------------------------------\n",
        "\n",
        "    for n_clusters in n_clusters_list:\n",
        "\n",
        "        kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
        "        clusters = kmeans.fit_predict(X_class)\n",
        "\n",
        "        for cluster_id in range(n_clusters):\n",
        "\n",
        "            idx_cluster = np.where(clusters == cluster_id)[0]\n",
        "\n",
        "            if len(idx_cluster) == 0:\n",
        "                continue\n",
        "\n",
        "            # Extract SHAP values for this cluster correctly\n",
        "            shap_vals_cluster = shap_vals_class[idx_cluster, :]  # Shape: (samples_in_cluster, features)\n",
        "\n",
        "            # Mean SHAP values\n",
        "            mean_abs_shap_cluster_average = np.abs(shap_vals_cluster).mean(axis=0)\n",
        "\n",
        "            # Save\n",
        "            key = (class_label, n_clusters, cluster_id)\n",
        "            feature_importances_by_class_cluster_average[key] = mean_abs_shap_cluster_average\n",
        "            feature_importances_by_class_cluster[key] = shap_vals_cluster\n"
      ],
      "metadata": {
        "id": "1SE4EJDNSyJ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nPlotting SHAP Feature Importance and Distribution Grid...\")\n",
        "\n",
        "# 1. Get class labels\n",
        "class_labels = list(label_encoder.classes_)\n",
        "n_classes = len(class_labels)\n",
        "\n",
        "# 2. Create 2 rows (importance / boxplot), n_classes columns\n",
        "fig, axes = plt.subplots(2, n_classes, figsize=(6 * n_classes, 12))\n",
        "\n",
        "if n_classes == 1:\n",
        "    axes = axes.reshape(2, 1)  # Fix for 1-class edge case\n",
        "\n",
        "# 3. First row: Absolute importance (mean abs SHAP)\n",
        "for idx, class_label in enumerate(class_labels):\n",
        "    ax = axes[0, idx]\n",
        "\n",
        "    feature_names_to_plot = feature_names[:len(feature_importances_by_class_average[class_label])]\n",
        "    shap_vals_to_plot = feature_importances_by_class_average[class_label]\n",
        "\n",
        "    ax.bar(\n",
        "        feature_names_to_plot,\n",
        "        shap_vals_to_plot,\n",
        "        color=[feature_colors[feat] for feat in feature_names_to_plot]\n",
        "    )\n",
        "    ax.set_title(f\"{class_label} - Mean |SHAP|\", fontsize=14)\n",
        "    ax.set_ylabel(\"Mean |SHAP| Value\", fontsize=12)\n",
        "    ax.set_xticklabels(feature_names_to_plot, rotation=45, ha='right')\n",
        "    ax.grid(True, axis='y')\n",
        "\n",
        "# 4. Second row: Distribution of SHAP values (Boxplot)\n",
        "for idx, class_label in enumerate(class_labels):\n",
        "    ax = axes[1, idx]\n",
        "\n",
        "    shap_vals_class = feature_importances_by_class[class_label]  # Full SHAP values (n_samples_in_class, n_features)\n",
        "\n",
        "    bp = ax.boxplot(\n",
        "        shap_vals_class,\n",
        "        vert=True,\n",
        "        labels=feature_names[:shap_vals_class.shape[1]],\n",
        "        patch_artist=True,\n",
        "        showfliers=True\n",
        "    )\n",
        "\n",
        "    # Optional: color boxes\n",
        "    box_colors = [feature_colors[feat] for feat in feature_names[:shap_vals_class.shape[1]]]\n",
        "    for patch, color in zip(bp['boxes'], box_colors):\n",
        "        patch.set_facecolor(color)\n",
        "\n",
        "    ax.axhline(0, color='black', linestyle='--')\n",
        "    ax.set_title(f\"{class_label} - SHAP Distribution (Boxplot)\", fontsize=14)\n",
        "    ax.set_ylabel(\"SHAP Value (Effect Direction)\", fontsize=12)\n",
        "    ax.set_xticklabels(feature_names[:shap_vals_class.shape[1]], rotation=45, ha='right')\n",
        "    ax.grid(True, axis='y')\n",
        "\n",
        "# Final layout\n",
        "fig.suptitle(\"Full Class SHAP Feature Importance and Effect Distribution\", fontsize=20)\n",
        "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "UoULaJRfq26a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example settings\n",
        "# class = [\"Duck\", \"Dog\", \"Cat\"] or [\"Zarnak\", \"Quorvian\", \"Bliptor\"]\n",
        "class_label = \"Bliptor\"\n",
        "# clasters : 2, 3, 4, 5\n",
        "n_clusters = 5\n",
        "feature_names_to_plot = feature_names\n",
        "\n",
        "# Colors for clusters\n",
        "cluster_colors = [\"#fd7f6f\", \"#7eb0d5\", \"#b2e061\", \"#bd7ebe\", \"#ffb55a\", \"#ffee65\", \"#beb9db\", \"#fdcce5\", \"#8bd3c7\"]\n",
        "\n",
        "\n",
        "# Prepare\n",
        "feature_importances_clusters = []\n",
        "for cluster_id in range(n_clusters):\n",
        "    key = (class_label, n_clusters, cluster_id)\n",
        "    if key in feature_importances_by_class_cluster_average:\n",
        "        feature_importances_clusters.append(feature_importances_by_class_cluster_average[key])\n",
        "\n",
        "feature_importances_clusters = np.array(feature_importances_clusters)\n",
        "\n",
        "# Plot\n",
        "x = np.arange(len(feature_names_to_plot))  # Feature index\n",
        "width = 0.15  # Bar width\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12, 6))\n",
        "\n",
        "for cluster_idx in range(n_clusters):\n",
        "    ax.bar(\n",
        "        x + cluster_idx * width,\n",
        "        feature_importances_clusters[cluster_idx],\n",
        "        width=width,\n",
        "        label=f\"Cluster {cluster_idx}\",\n",
        "        color=cluster_colors[cluster_idx % len(cluster_colors)]  # Color by cluster\n",
        "    )\n",
        "\n",
        "# X-axis setup\n",
        "ax.set_xticks(x + width * (n_clusters-1)/2)\n",
        "ax.set_xticklabels(feature_names_to_plot, rotation=45, ha='right')\n",
        "\n",
        "# Labels and Title\n",
        "ax.set_ylabel(\"Mean |SHAP| Value\", fontsize=12)\n",
        "ax.set_title(f\"Feature Importance Across Clusters for Class '{class_label}'\", fontsize=16)\n",
        "ax.grid(True, axis='y')\n",
        "ax.legend(title=\"Cluster\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "cjqvg071S5B4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ§  Anchor Explanations"
      ],
      "metadata": {
        "id": "k86Da20BTIHt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nRunning Anchor explanations...\")\n",
        "\n",
        "# Create anchor explainer\n",
        "predict_fn = lambda x: model.predict(x)\n",
        "anchor_explainer = AnchorTabular(predict_fn, feature_names=X.columns.tolist())\n",
        "anchor_explainer.fit(X_scaled)\n",
        "\n",
        "n_anchors_per_class = 3\n",
        "\n",
        "for class_idx, class_label in enumerate(label_encoder.classes_):\n",
        "    print(f\"\\nAnchors for class: {class_label}\")\n",
        "    idx_class = np.where(y_pred == class_idx)[0]\n",
        "\n",
        "    for j in range(min(n_anchors_per_class, len(idx_class))):\n",
        "        i = np.random.choice(idx_class)\n",
        "        sample_scaled = X_scaled[i].reshape(1, -1)\n",
        "\n",
        "        # Inverse transform sample to original scale\n",
        "        sample_original = scaler.inverse_transform(sample_scaled)\n",
        "\n",
        "        # Explain\n",
        "        explanation = anchor_explainer.explain(sample_scaled)\n",
        "\n",
        "        print(f\"\\nAnchor {j+1} for class {class_label}:\")\n",
        "\n",
        "        # Rebuild human-readable conditions\n",
        "        readable_conditions = []\n",
        "\n",
        "        # inside your loop\n",
        "        for cond in explanation.anchor:\n",
        "            # Try to parse\n",
        "            match = re.match(r\"([a-zA-Z_ ]+) ([<>=]+) ([\\d\\.\\-eE]+)\", cond.strip())\n",
        "            if match:\n",
        "                feature_name, operator, threshold_scaled_str = match.groups()\n",
        "                threshold_scaled = float(threshold_scaled_str)\n",
        "\n",
        "                feature_idx = list(X.columns).index(feature_name)\n",
        "\n",
        "                dummy = np.zeros((1, X_scaled.shape[1]))\n",
        "                dummy[0, feature_idx] = threshold_scaled\n",
        "                threshold_original = scaler.inverse_transform(dummy)[0, feature_idx]\n",
        "\n",
        "                readable_conditions.append(f\"{feature_name} {operator} {threshold_original:.2f}\")\n",
        "            else:\n",
        "                print(f\"Warning: Could not parse condition: {cond}\")\n",
        "                continue\n",
        "\n",
        "\n",
        "        print('  Conditions:', readable_conditions)\n",
        "        print('  Precision:', explanation.precision)\n",
        "        print('  Coverage:', explanation.coverage)\n"
      ],
      "metadata": {
        "id": "_5IP__FRTGhC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ“œ Rule Extraction"
      ],
      "metadata": {
        "id": "wxreluolUUmF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---- Start the Rule Extraction Process ----\n",
        "print(\"\\nExtracting Rules...\")\n",
        "\n",
        "# Define different extraction settings: Light, Medium, Strict\n",
        "settings = [\n",
        "    {'name': 'Light', 'n_trees': 5, 'threshold': 0.3},   # Light: Few trees, low frequency threshold\n",
        "    {'name': 'Medium', 'n_trees': 10, 'threshold': 0.5}, # Medium: More trees, higher frequency threshold\n",
        "    {'name': 'Strict', 'n_trees': 20, 'threshold': 0.7}, # Strict: Many trees, strict frequency threshold\n",
        "]\n",
        "\n",
        "# Loop over each setting\n",
        "for setting in settings:\n",
        "    print(f\"\\n### Setting: {setting['name']} ###\")\n",
        "    n_trees = setting['n_trees']\n",
        "    threshold = setting['threshold']\n",
        "\n",
        "    # Loop over each class label (e.g., class names like 'Apple', 'Banana', etc.)\n",
        "    for class_idx, class_label in enumerate(label_encoder.classes_):\n",
        "        print(f\"\\nClass: {class_label}\")\n",
        "\n",
        "        # ---- SIMPLE RULES: Show rules from individual trees ----\n",
        "        print(\"\\nSimple Rules (Single Trees):\")\n",
        "        for i in range(min(n_trees, len(model.estimators_))):\n",
        "            # Export and print the decision rules from tree i\n",
        "            tree_rules = export_text(model.estimators_[i], feature_names=list(X.columns))\n",
        "            print(f\"\\nTree {i} Rules:\\n\", tree_rules)\n",
        "\n",
        "        # ---- AGGREGATED RULES: Combine rules across multiple trees ----\n",
        "        print(\"\\nAggregated Rules (Across Forest):\")\n",
        "\n",
        "        feature_counts = {}  # Dictionary to count how often each feature condition appears\n",
        "        path_counts = [0]    # List with one item: total number of paths predicting this class\n",
        "\n",
        "        # Loop over the first n_trees estimators\n",
        "        for estimator in model.estimators_[:n_trees]:\n",
        "            tree = estimator.tree_\n",
        "            feature = tree.feature\n",
        "            threshold_values = tree.threshold\n",
        "\n",
        "            # Recursive function to walk through a tree\n",
        "            def traverse(node=0, conditions=[]):\n",
        "                # If we reach a leaf node\n",
        "                if tree.children_left[node] == tree.children_right[node]:\n",
        "                    if np.argmax(tree.value[node]) == class_idx:\n",
        "                        # If the leaf predicts the current class, count its conditions\n",
        "                        for cond in conditions:\n",
        "                            feature_counts[cond] = feature_counts.get(cond, 0) + 1\n",
        "                        path_counts[0] += 1  # Count this path\n",
        "                    return\n",
        "\n",
        "                # If it's a decision node\n",
        "                feat = feature[node]\n",
        "                thresh = threshold_values[node]\n",
        "                descaled = thresh * scaler.scale_[feat] + scaler.mean_[feat]\n",
        "\n",
        "\n",
        "\n",
        "                # Recursively explore left and right branches, adding conditions\n",
        "\n",
        "                if feat >= 0:\n",
        "                    # Recursively explore left and right branches, adding conditions\n",
        "                    traverse(\n",
        "                        tree.children_left[node],\n",
        "                        conditions + [f\"{X.columns[feat]} <= {descaled:.2f}\"]\n",
        "                    )\n",
        "                    traverse(\n",
        "                        tree.children_right[node],\n",
        "                        conditions + [f\"{X.columns[feat]} > {descaled:.2f}\"]\n",
        "                    )\n",
        "\n",
        "            # Start traversing from the root node\n",
        "            traverse()\n",
        "\n",
        "        # After traversing all trees, print conditions that appear frequently enough\n",
        "        for cond, count in feature_counts.items():\n",
        "            freq = count / path_counts[0] if path_counts[0] else 0\n",
        "            if freq >= threshold:\n",
        "                print(f\"{cond} appeared in {freq*100:.1f}% of paths.\")\n"
      ],
      "metadata": {
        "id": "o38lXwnCUUy2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ---- Start the Rule Extraction Process ----\n",
        "print(\"\\nExtracting Rules...\")\n",
        "\n",
        "# Define output file\n",
        "output_file = \"extracted_rules.txt\"\n",
        "\n",
        "# Clear the file before writing\n",
        "with open(output_file, \"w\") as f:\n",
        "    f.write(\"\")\n",
        "\n",
        "# Define different extraction settings: Light, Medium, Strict\n",
        "settings = [\n",
        "    {'name': 'Light', 'n_trees': 5, 'threshold': 0.3},\n",
        "    {'name': 'Medium', 'n_trees': 10, 'threshold': 0.5},\n",
        "    {'name': 'Strict', 'n_trees': 20, 'threshold': 0.7},\n",
        "]\n",
        "\n",
        "# Feature names list\n",
        "feature_names = list(X.columns)\n",
        "\n",
        "# Helper function to log text to file and console\n",
        "def log(text):\n",
        "    print(text)\n",
        "    with open(output_file, \"a\") as f:\n",
        "        f.write(text + \"\\n\")\n",
        "\n",
        "# Loop over each setting\n",
        "for setting in settings:\n",
        "    log(f\"\\n### Setting: {setting['name']} (n_trees={setting['n_trees']}, threshold={setting['threshold']}) ###\")\n",
        "    n_trees = setting['n_trees']\n",
        "    threshold = setting['threshold']\n",
        "\n",
        "    # Loop over each class label\n",
        "    for class_idx, class_label in enumerate(label_encoder.classes_):\n",
        "        log(f\"\\nClass: {class_label}\")\n",
        "\n",
        "        # ---- SIMPLE RULES: Show rules from individual trees ----\n",
        "        log(\"\\nSimple Rules (Single Trees):\")\n",
        "        for i in range(min(n_trees, len(model.estimators_))):\n",
        "            estimator = model.estimators_[i]\n",
        "            tree = estimator.tree_\n",
        "            feature = tree.feature\n",
        "            threshold_values = tree.threshold\n",
        "\n",
        "            def print_rules(node=0, depth=0):\n",
        "                indent = \"  \" * depth\n",
        "                if tree.children_left[node] == tree.children_right[node]:\n",
        "                    predicted_class_idx = np.argmax(tree.value[node])\n",
        "                    predicted_class = label_encoder.classes_[predicted_class_idx]\n",
        "                    log(indent + f\"Predict: {predicted_class}\")\n",
        "                    return\n",
        "\n",
        "                feat = feature[node]\n",
        "                thresh = threshold_values[node]\n",
        "\n",
        "                # Properly descale\n",
        "                if isinstance(scaler, StandardScaler):\n",
        "                    descaled_thresh = thresh * scaler.scale_[feat] + scaler.mean_[feat]\n",
        "                else:\n",
        "                    descaled_thresh = thresh\n",
        "\n",
        "                if feat >= 0:\n",
        "                    log(indent + f\"if {feature_names[feat]} <= {descaled_thresh:.2f}:\")\n",
        "                    print_rules(tree.children_left[node], depth + 1)\n",
        "                    log(indent + f\"else ({feature_names[feat]} > {descaled_thresh:.2f}):\")\n",
        "                    print_rules(tree.children_right[node], depth + 1)\n",
        "\n",
        "            log(f\"\\nTree {i} Rules:\")\n",
        "            print_rules()\n",
        "\n",
        "        # ---- AGGREGATED RULES: Combine rules across multiple trees ----\n",
        "        log(\"\\nAggregated Rules (Across Forest):\")\n",
        "\n",
        "        feature_counts = {}  # Dictionary to count how often each feature condition appears\n",
        "        path_counts = [0]    # List with one item: total number of paths predicting this class\n",
        "\n",
        "        # Loop over the first n_trees estimators\n",
        "        for estimator in model.estimators_[:n_trees]:\n",
        "            tree = estimator.tree_\n",
        "            feature = tree.feature\n",
        "            threshold_values = tree.threshold\n",
        "\n",
        "            # Recursive function to walk through a tree\n",
        "            def traverse(node=0, conditions=[]):\n",
        "                if tree.children_left[node] == tree.children_right[node]:\n",
        "                    if np.argmax(tree.value[node]) == class_idx:\n",
        "                        for cond in conditions:\n",
        "                            feature_counts[cond] = feature_counts.get(cond, 0) + 1\n",
        "                        path_counts[0] += 1\n",
        "                    return\n",
        "\n",
        "                feat = feature[node]\n",
        "                thresh = threshold_values[node]\n",
        "\n",
        "                if feat >= 0:\n",
        "                    # Properly descale\n",
        "                    if isinstance(scaler, StandardScaler):\n",
        "                        descaled_thresh = thresh * scaler.scale_[feat] + scaler.mean_[feat]\n",
        "                    else:\n",
        "                        descaled_thresh = thresh\n",
        "\n",
        "                    traverse(\n",
        "                        tree.children_left[node],\n",
        "                        conditions + [f\"{feature_names[feat]} <= {descaled_thresh:.2f}\"]\n",
        "                    )\n",
        "                    traverse(\n",
        "                        tree.children_right[node],\n",
        "                        conditions + [f\"{feature_names[feat]} > {descaled_thresh:.2f}\"]\n",
        "                    )\n",
        "\n",
        "            traverse()\n",
        "\n",
        "        # After traversing all trees, print conditions that appear frequently enough\n",
        "        for cond, count in feature_counts.items():\n",
        "            freq = count / path_counts[0] if path_counts[0] else 0\n",
        "            if freq >= threshold:\n",
        "                log(f\"{cond} appeared in {freq*100:.1f}% of paths.\")\n",
        "\n",
        "log(\"\\nFinished rule extraction. Output saved to 'extracted_rules.txt'.\")\n"
      ],
      "metadata": {
        "id": "15Mia2173BeN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PDP (Partial Dependency Plot)"
      ],
      "metadata": {
        "id": "MSgtC21W-a6P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_pdp_grid_for_class(model, X_scaled, feature_names, label_encoder, class_label, scaler, grid_resolution=50):\n",
        "    \"\"\"\n",
        "    Plots PDP+ICE (individual) and PDP (average) for all features, side-by-side per feature.\n",
        "    \"\"\"\n",
        "\n",
        "    print(f\"Plotting PDP grid for class '{class_label}'...\")\n",
        "\n",
        "    class_idx = list(label_encoder.classes_).index(class_label)\n",
        "    n_features = len(feature_names)\n",
        "\n",
        "    fig, axes = plt.subplots(nrows=n_features, ncols=2, figsize=(14, 3 * n_features))\n",
        "\n",
        "    if n_features == 1:\n",
        "        axes = axes.reshape(1, 2)\n",
        "\n",
        "    for feature_idx, feature_name in enumerate(feature_names):\n",
        "\n",
        "        # --- LEFT: PDP + ICE ---\n",
        "        display = PartialDependenceDisplay.from_estimator(\n",
        "            model,\n",
        "            X_scaled,\n",
        "            features=[feature_idx],\n",
        "            target=class_idx,\n",
        "            feature_names=feature_names,\n",
        "            kind='individual',\n",
        "            grid_resolution=grid_resolution,\n",
        "            ax=axes[feature_idx, 0],\n",
        "        )\n",
        "\n",
        "        ax_left = display.axes_[0, 0]\n",
        "        x_ticks_scaled = ax_left.get_xticks()\n",
        "\n",
        "        x_dummy = np.zeros((len(x_ticks_scaled), X_scaled.shape[1]))\n",
        "        x_dummy[:, feature_idx] = x_ticks_scaled\n",
        "        x_ticks_original = scaler.inverse_transform(x_dummy)[:, feature_idx]\n",
        "\n",
        "        ax_left.set_xticks(x_ticks_scaled)\n",
        "        ax_left.set_xticklabels([f\"{tick:.2f}\" for tick in x_ticks_original])\n",
        "\n",
        "        ax_left.set_ylabel(\"Partial Dependence (Individual)\", fontsize=10)\n",
        "        ax_left.set_xlabel(f\"{feature_name} (Original Scale)\", fontsize=10)\n",
        "        ax_left.set_title(f\"{feature_name} - PDP + ICE\", fontsize=12)\n",
        "        ax_left.grid(True)\n",
        "\n",
        "        # --- RIGHT: PDP Average only ---\n",
        "        display = PartialDependenceDisplay.from_estimator(\n",
        "            model,\n",
        "            X_scaled,\n",
        "            features=[feature_idx],\n",
        "            target=class_idx,\n",
        "            feature_names=feature_names,\n",
        "            kind='average',\n",
        "            grid_resolution=grid_resolution,\n",
        "            ax=axes[feature_idx, 1],\n",
        "        )\n",
        "\n",
        "        ax_right = display.axes_[0, 0]\n",
        "        x_ticks_scaled = ax_right.get_xticks()\n",
        "\n",
        "        x_dummy = np.zeros((len(x_ticks_scaled), X_scaled.shape[1]))\n",
        "        x_dummy[:, feature_idx] = x_ticks_scaled\n",
        "        x_ticks_original = scaler.inverse_transform(x_dummy)[:, feature_idx]\n",
        "\n",
        "        ax_right.set_xticks(x_ticks_scaled)\n",
        "        ax_right.set_xticklabels([f\"{tick:.2f}\" for tick in x_ticks_original])\n",
        "\n",
        "        ax_right.set_ylabel(\"Partial Dependence (Average)\", fontsize=10)\n",
        "        ax_right.set_xlabel(f\"{feature_name} (Original Scale)\", fontsize=10)\n",
        "        ax_right.set_title(f\"{feature_name} - PDP Average\", fontsize=12)\n",
        "        ax_right.grid(True)\n",
        "\n",
        "    fig.suptitle(f\"Partial Dependence for Class '{class_label}'\", fontsize=18)\n",
        "    plt.tight_layout(rect=[0, 0.03, 1, 0.97])\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# -----\n",
        "\n",
        "def plot_2d_pdp_on_ax(model, X_scaled, feature_names, label_encoder, class_label, feature1, feature2, scaler, ax, grid_resolution=30):\n",
        "    \"\"\"\n",
        "    Plots a 2D PDP for two features on a given matplotlib axis.\n",
        "    \"\"\"\n",
        "\n",
        "    class_idx = list(label_encoder.classes_).index(class_label)\n",
        "\n",
        "    feature1_idx = list(feature_names).index(feature1)\n",
        "    feature2_idx = list(feature_names).index(feature2)\n",
        "\n",
        "    display = PartialDependenceDisplay.from_estimator(\n",
        "        model,\n",
        "        X_scaled,\n",
        "        features=[(feature1_idx, feature2_idx)],\n",
        "        target=class_idx,\n",
        "        feature_names=feature_names,\n",
        "        kind='average',\n",
        "        grid_resolution=grid_resolution,\n",
        "        ax=ax,\n",
        "    )\n",
        "\n",
        "    ax_disp = display.axes_[0, 0]\n",
        "\n",
        "    # Fix X ticks\n",
        "    x_ticks_scaled = ax_disp.get_xticks()\n",
        "    x_dummy = np.zeros((len(x_ticks_scaled), X_scaled.shape[1]))\n",
        "    x_dummy[:, feature1_idx] = x_ticks_scaled\n",
        "    x_ticks_original = scaler.inverse_transform(x_dummy)[:, feature1_idx]\n",
        "\n",
        "    ax_disp.set_xticks(x_ticks_scaled)\n",
        "    ax_disp.set_xticklabels([f\"{tick:.2f}\" for tick in x_ticks_original])\n",
        "\n",
        "    # Fix Y ticks\n",
        "    y_ticks_scaled = ax_disp.get_yticks()\n",
        "    y_dummy = np.zeros((len(y_ticks_scaled), X_scaled.shape[1]))\n",
        "    y_dummy[:, feature2_idx] = y_ticks_scaled\n",
        "    y_ticks_original = scaler.inverse_transform(y_dummy)[:, feature2_idx]\n",
        "\n",
        "    ax_disp.set_yticks(y_ticks_scaled)\n",
        "    ax_disp.set_yticklabels([f\"{tick:.2f}\" for tick in y_ticks_original])\n",
        "\n",
        "    ax_disp.set_xlabel(f\"{feature1} (original scale)\", fontsize=8)\n",
        "    ax_disp.set_ylabel(f\"{feature2} (original scale)\", fontsize=8)\n",
        "    ax_disp.set_title(f\"{feature1} vs {feature2}\", fontsize=9)\n",
        "    ax_disp.tick_params(axis='both', labelsize=6)\n",
        "    ax_disp.grid(True)\n",
        "#------\n",
        "\n",
        "def plot_2d_pdp_grid(model, X_scaled, feature_names, label_encoder, class_label, scaler):\n",
        "    \"\"\"\n",
        "    Plots a full 5x5 grid of 2D PDPs for all feature pairs.\n",
        "    \"\"\"\n",
        "\n",
        "    n_features = len(feature_names)\n",
        "    fig, axes = plt.subplots(nrows=n_features, ncols=n_features, figsize=(3*n_features, 3*n_features))\n",
        "\n",
        "    if n_features == 1:\n",
        "        axes = axes.reshape(1, 1)\n",
        "\n",
        "    for i, feature1 in enumerate(feature_names):\n",
        "        for j, feature2 in enumerate(feature_names):\n",
        "            ax = axes[i, j]\n",
        "\n",
        "            if i == j:\n",
        "                ax.axis('off')\n",
        "                ax.text(0.5, 0.5, feature1, horizontalalignment='center', verticalalignment='center', fontsize=10)\n",
        "            else:\n",
        "                plot_2d_pdp_on_ax(\n",
        "                    model=model,\n",
        "                    X_scaled=X_scaled,\n",
        "                    feature_names=feature_names,\n",
        "                    label_encoder=label_encoder,\n",
        "                    class_label=class_label,\n",
        "                    feature1=feature1,\n",
        "                    feature2=feature2,\n",
        "                    scaler=scaler,\n",
        "                    ax=ax,\n",
        "                    grid_resolution=30,\n",
        "                )\n",
        "\n",
        "    fig.suptitle(f\"2D Partial Dependence Grid for Class '{class_label}'\", fontsize=20)\n",
        "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "R5Ag4ON9-Yo1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# class = [\"Duck\", \"Dog\", \"Cat\"] or [\"Zarnak\", \"Quorvian\", \"Bliptor\"]\n",
        "class_label = \"Cat\"\n",
        "plot_pdp_grid_for_class(\n",
        "    model=model,\n",
        "    X_scaled=X_scaled,\n",
        "    feature_names=feature_names,\n",
        "    label_encoder=label_encoder,\n",
        "    class_label=class_label,\n",
        "    scaler=scaler,\n",
        "    grid_resolution=50\n",
        ")\n",
        "\n",
        "plot_2d_pdp_grid(\n",
        "    model=model,\n",
        "    X_scaled=X_scaled,\n",
        "    feature_names=feature_names,\n",
        "    label_encoder=label_encoder,\n",
        "    class_label=class_label,\n",
        "    scaler=scaler\n",
        ")"
      ],
      "metadata": {
        "id": "0WnirE9NRLMy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}