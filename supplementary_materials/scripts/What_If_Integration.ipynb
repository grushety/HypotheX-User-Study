{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MX9kySB2N2pZ"
      },
      "outputs": [],
      "source": [
        "!pip install protobuf==3.20.3 witwidget tensorflow catboost --quiet\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from witwidget.notebook.visualization import WitWidget, WitConfigBuilder\n",
        "from google.colab import files\n",
        "import joblib  # For loading trained models\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload dataset\n",
        "uploaded_dataset = files.upload()\n",
        "\n",
        "# Identify dataset file\n",
        "dataset_filename = None\n",
        "\n",
        "for filename in uploaded_dataset.keys():\n",
        "    if filename.endswith(\".csv\"):  # Detects dataset file\n",
        "        dataset_filename = filename\n",
        "\n",
        "# Load the dataset if found\n",
        "if dataset_filename:\n",
        "    df = pd.read_csv(dataset_filename)\n",
        "    df_test = df.copy()\n",
        "    print(f\"✅ Dataset '{dataset_filename}' has been successfully loaded.\")\n",
        "else:\n",
        "    print(\"⚠️ No dataset file (.csv) found. Please upload a valid dataset.\")"
      ],
      "metadata": {
        "id": "-iDNenvnOUsy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload model\n",
        "uploaded_model = files.upload()\n",
        "\n",
        "# Identify model file\n",
        "model_filename = None\n",
        "\n",
        "for filename in uploaded_model.keys():\n",
        "    if filename.endswith(\".pkl\"):  # Detects model file\n",
        "        model_filename = filename\n",
        "\n",
        "# Load the model if found\n",
        "if model_filename:\n",
        "    model = joblib.load(model_filename)\n",
        "    print(f\"✅ Model '{model_filename}' has been successfully loaded.\")\n",
        "else:\n",
        "    print(\"⚠️ No model file (.pkl) found. Please upload a valid model.\")"
      ],
      "metadata": {
        "id": "OUZo-5Z9RV8S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload scaler file\n",
        "uploaded_scaler = files.upload()\n",
        "\n",
        "# Identify scaler file dynamically\n",
        "scaler_filename = None\n",
        "\n",
        "for filename in uploaded_scaler.keys():\n",
        "    if \"scaler\" in filename and filename.endswith(\".pkl\"):\n",
        "        scaler_filename = filename\n",
        "\n",
        "# Load scaler if found\n",
        "scaler = None\n",
        "\n",
        "if scaler_filename:\n",
        "    scaler = joblib.load(scaler_filename)\n",
        "    print(f\"✅ Scaler '{scaler_filename}' has been successfully loaded.\")\n",
        "else:\n",
        "    print(\"⚠️ No scaler file (.pkl) found. Skipping scaling.\")\n",
        "\n",
        "if scaler:\n",
        "    # Extract feature columns (excluding \"Class\")\n",
        "    feature_columns = [col for col in df.columns if col != \"Class\"]\n",
        "\n",
        "    # Apply scaling to features\n",
        "    X_scaled = scaler.transform(df[feature_columns])\n",
        "    df_scaled = pd.DataFrame(X_scaled, columns=feature_columns)  # Convert back to DataFrame\n",
        "\n",
        "    print(\"✅ Dataset features have been successfully scaled.\")\n",
        "else:\n",
        "    print(\"⚠️ Skipping scaling as no scaler file was found.\")\n"
      ],
      "metadata": {
        "id": "tEngjg3YXZ_q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload label encoder file\n",
        "uploaded_encoder = files.upload()\n",
        "\n",
        "# Identify label encoder file dynamically\n",
        "label_encoder_filename = None\n",
        "\n",
        "for filename in uploaded_encoder.keys():\n",
        "    if \"label_encoder\" in filename and filename.endswith(\".pkl\"):\n",
        "        label_encoder_filename = filename\n",
        "\n",
        "# Load label encoder if found\n",
        "label_encoder = None\n",
        "\n",
        "if label_encoder_filename:\n",
        "    label_encoder = joblib.load(label_encoder_filename)\n",
        "    print(f\"✅ Label Encoder '{label_encoder_filename}' has been successfully loaded.\")\n",
        "else:\n",
        "    print(\"⚠️ No label encoder file (.pkl) found. Skipping encoding.\")\n",
        "\n",
        "if label_encoder:\n",
        "    df[\"Class\"] = label_encoder.transform(df[\"Class\"])\n",
        "    print(\"✅ Class labels have been successfully encoded.\")\n",
        "else:\n",
        "    print(\"⚠️ Skipping encoding as no label encoder file was found.\")\n"
      ],
      "metadata": {
        "id": "OHn0-zN_X1_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure feature_columns are correctly defined\n",
        "feature_columns = [col for col in df.columns if col != \"Class\"]\n",
        "\n",
        "# Convert dataset into list format (WIT requires JSON-like structure)\n",
        "instances = df_scaled.to_dict(orient=\"records\")  # Use scaled dataset\n",
        "\n",
        "# Define class mapping based on encoded values\n",
        "class_mapping = {i: label for i, label in enumerate(label_encoder.classes_)}\n",
        "print(\"Class Mapping:\", class_mapping)\n",
        "\n",
        "# Debugging: Print test predictions using scaled data\n",
        "test_prediction = model.predict(df_scaled.iloc[:10])  # Use scaled data\n",
        "print(\"Raw Model Predictions:\", test_prediction)\n",
        "print(df_scaled.iloc[:10])  # Show scaled feature values\n",
        "print(df.head(10))  # Show unscaled data for comparison\n",
        "print(instances[0])  # Show JSON-like instance structure\n",
        "\n",
        "if \"Class\" in df_test.columns:\n",
        "          inputs_df = df_test.drop(columns=[\"Class\"])\n",
        "instances = df_test.to_dict(orient=\"records\")\n",
        "print(type(instances))\n",
        "print(type(instances[0]))\n",
        "print(instances[0])"
      ],
      "metadata": {
        "id": "7WrHiyKwOwcX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a wrapper function for WIT\n",
        "class WitModelWrapper:\n",
        "    def __init__(self, model, class_mapping, scaler):\n",
        "        self.model = model\n",
        "        self.class_mapping = class_mapping\n",
        "        self.scaler = scaler  # Store scaler for prediction\n",
        "\n",
        "    def predict(self, inputs):\n",
        "\n",
        "        if isinstance(inputs, dict):  # Ensure inputs is a list of dictionaries\n",
        "          inputs = [inputs]\n",
        "\n",
        "        if not all(isinstance(i, dict) for i in inputs):\n",
        "          raise ValueError(\"❌ Invalid input format! Expected a list of dictionaries.\")\n",
        "\n",
        "        # Convert inputs to DataFrame\n",
        "        inputs_df = pd.DataFrame(inputs)\n",
        "        # Drop the \"Class\" column if it exists in inputs\n",
        "        if \"Class\" in inputs_df.columns:\n",
        "          inputs_df = inputs_df.drop(columns=[\"Class\"])\n",
        "\n",
        "        # Ensure correct column order\n",
        "        inputs_df = inputs_df[feature_columns]\n",
        "\n",
        "        # Apply scaling before prediction\n",
        "        inputs_scaled = self.scaler.transform(inputs_df)\n",
        "        print(\"Scaled inputs:\", inputs_scaled)\n",
        "\n",
        "        # Get predictions from the model\n",
        "        preds = self.model.predict(inputs_scaled)\n",
        "        # Get model predictions (numeric output required)\n",
        "        preds = self.model.predict_proba(inputs_scaled)  # ✅ Use predict_proba() instead of predict()\n",
        "\n",
        "\n",
        "        # Convert predictions back to class names\n",
        "        #categorical_preds = [self.class_mapping[int(pred)] for pred in preds]\n",
        "\n",
        "        print(\"Predictions:\", preds)  # Debugging output\n",
        "        return preds.tolist()  # Return categorical predictions\n",
        "\n",
        "# Wrap the trained model with scaling for WIT predictions\n",
        "wrapped_model = WitModelWrapper(model, class_mapping, scaler)\n",
        "\n",
        "# Debugging: Test wrapped model prediction\n",
        "print(\"Test Wrapped Model Prediction:\", wrapped_model.predict([instances[0]]))\n"
      ],
      "metadata": {
        "id": "YQkoYCYXOxxf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define feature spec for WIT\n",
        "feature_spec = {feature: tf.io.FixedLenFeature([], tf.float32) for feature in feature_columns}\n",
        "feature_spec[\"Class\"] = tf.io.FixedLenFeature([], tf.string)  # Ensure \"Class\" is a string category\n",
        "\n",
        "# Define class labels\n",
        "class_labels = list(class_mapping.values())  # Get class names from mapping\n",
        "\n",
        "config_builder = WitConfigBuilder(instances) \\\n",
        "    .set_custom_predict_fn(wrapped_model.predict) \\\n",
        "    .set_label_vocab(class_labels) \\\n",
        "    .set_target_feature(\"Class\")\n",
        "\n",
        "\n",
        "# Launch WIT in the notebook\n",
        "WitWidget(config_builder)"
      ],
      "metadata": {
        "id": "nxE01ITRO4LH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}